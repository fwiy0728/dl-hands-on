{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0b4e435",
   "metadata": {},
   "source": [
    "# MoveNet Lightningã«ã‚ˆã‚‹å§¿å‹¢æ¨å®š - Google Colabç‰ˆ\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€TensorFlow Hubã®MoveNet Lightningãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å§¿å‹¢æ¨å®šã‚’è¡Œã„ã¾ã™ã€‚MoveNet Lightningã¯è»½é‡ã§é«˜é€Ÿãªå§¿å‹¢æ¨å®šãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚\n",
    "\n",
    "## ç‰¹å¾´\n",
    "- é«˜é€Ÿãªãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å§¿å‹¢æ¨å®š\n",
    "- 17å€‹ã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®æ¤œå‡º\n",
    "- è»½é‡ãƒ¢ãƒ‡ãƒ«ï¼ˆLightningç‰ˆï¼‰\n",
    "- ç”»åƒãƒ»å‹•ç”»ä¸¡æ–¹ã«å¯¾å¿œ\n",
    "- ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ä»˜ãã®çµæœ\n",
    "\n",
    "## å¿…è¦ãªç’°å¢ƒ\n",
    "- Google Colabï¼ˆæ¨å¥¨ï¼‰\n",
    "- Python 3.8ä»¥ä¸Š\n",
    "- TensorFlow 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d38d25",
   "metadata": {},
   "source": [
    "## 1. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "\n",
    "MoveNet Lightningã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã«å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528f3176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlowã¨TensorFlow Hubã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!pip install tensorflow tensorflow-hub\n",
    "\n",
    "# ãã®ä»–ã®å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!pip install opencv-python-headless pillow matplotlib numpy\n",
    "\n",
    "# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†ã®ç¢ºèª\n",
    "print(\"âœ… å…¨ã¦ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae8316b",
   "metadata": {},
   "source": [
    "## 2. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨è¨­å®š\n",
    "\n",
    "å§¿å‹¢æ¨å®šã«å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7da0552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "from google.colab import files\n",
    "import io\n",
    "\n",
    "# GPUåˆ©ç”¨å¯èƒ½æ€§ã®ç¢ºèª\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(\"âœ… å…¨ã¦ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf4d4cd",
   "metadata": {},
   "source": [
    "## 3. MoveNet Lightningãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "\n",
    "TensorFlow Hubã‹ã‚‰MoveNet Lightningãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a25c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MoveNet Lightning ãƒ¢ãƒ‡ãƒ«ã‚’TensorFlow Hubã‹ã‚‰èª­ã¿è¾¼ã¿\n",
    "model_url = \"https://tfhub.dev/google/movenet/singlepose/lightning/4\"\n",
    "print(\"ğŸ”„ MoveNet Lightningãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "movenet = hub.load(model_url)\n",
    "movenet_fn = movenet.signatures['serving_default']\n",
    "\n",
    "print(\"âœ… MoveNet Lightningãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
    "print(\"ğŸ“Š ãƒ¢ãƒ‡ãƒ«æƒ…å ±:\")\n",
    "print(\"   - å…¥åŠ›ã‚µã‚¤ã‚º: 192x192\")\n",
    "print(\"   - æ¤œå‡ºã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ•°: 17å€‹\")\n",
    "print(\"   - å‡ºåŠ›å½¢å¼: [y, x, confidence] Ã— 17\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab3b756",
   "metadata": {},
   "source": [
    "## 4. ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆå®šç¾©ã¨æç”»é–¢æ•°\n",
    "\n",
    "MoveNetãŒæ¤œå‡ºã™ã‚‹17å€‹ã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®å®šç¾©ã¨ã€çµæœã‚’å¯è¦–åŒ–ã™ã‚‹ãŸã‚ã®é–¢æ•°ã‚’å®šç¾©ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69202987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MoveNetã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆå®šç¾©ï¼ˆ17å€‹ï¼‰\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "# ã‚¹ã‚±ãƒ«ãƒˆãƒ³ã®æ¥ç¶šé–¢ä¿‚\n",
    "CONNECTIONS = [\n",
    "    # é¡”\n",
    "    (0, 1), (0, 2), (1, 3), (2, 4),\n",
    "    # èƒ´ä½“\n",
    "    (5, 6), (5, 11), (6, 12), (11, 12),\n",
    "    # å·¦è…•\n",
    "    (5, 7), (7, 9),\n",
    "    # å³è…•\n",
    "    (6, 8), (8, 10),\n",
    "    # å·¦è„š\n",
    "    (11, 13), (13, 15),\n",
    "    # å³è„š\n",
    "    (12, 14), (14, 16)\n",
    "]\n",
    "\n",
    "def draw_keypoints_and_skeleton(image, keypoints, confidence_threshold=0.3):\n",
    "    \"\"\"\n",
    "    ç”»åƒã«ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã¨ã‚¹ã‚±ãƒ«ãƒˆãƒ³ã‚’æç”»ã™ã‚‹é–¢æ•°\n",
    "    \"\"\"\n",
    "    image_copy = image.copy()\n",
    "    height, width = image_copy.shape[:2]\n",
    "    \n",
    "    # ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã‚’æç”»\n",
    "    for i, (y, x, conf) in enumerate(keypoints):\n",
    "        if conf > confidence_threshold:\n",
    "            # åº§æ¨™ã‚’ç”»åƒã‚µã‚¤ã‚ºã«ã‚¹ã‚±ãƒ¼ãƒ«\n",
    "            x_coord = int(x * width)\n",
    "            y_coord = int(y * height)\n",
    "            \n",
    "            # ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã‚’å††ã§æç”»\n",
    "            cv2.circle(image_copy, (x_coord, y_coord), 5, (0, 255, 0), -1)\n",
    "            cv2.circle(image_copy, (x_coord, y_coord), 7, (0, 0, 255), 2)\n",
    "    \n",
    "    # ã‚¹ã‚±ãƒ«ãƒˆãƒ³ã‚’æç”»\n",
    "    for connection in CONNECTIONS:\n",
    "        point1_idx, point2_idx = connection\n",
    "        \n",
    "        y1, x1, conf1 = keypoints[point1_idx]\n",
    "        y2, x2, conf2 = keypoints[point2_idx]\n",
    "        \n",
    "        if conf1 > confidence_threshold and conf2 > confidence_threshold:\n",
    "            x1_coord = int(x1 * width)\n",
    "            y1_coord = int(y1 * height)\n",
    "            x2_coord = int(x2 * width)\n",
    "            y2_coord = int(y2 * height)\n",
    "            \n",
    "            cv2.line(image_copy, (x1_coord, y1_coord), (x2_coord, y2_coord), (255, 0, 0), 2)\n",
    "    \n",
    "    return image_copy\n",
    "\n",
    "print(\"âœ… ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆå®šç¾©ã¨æç”»é–¢æ•°ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cc6526",
   "metadata": {},
   "source": [
    "## 5. å§¿å‹¢æ¨å®šå®Ÿè¡Œé–¢æ•°\n",
    "\n",
    "ç”»åƒã«å¯¾ã—ã¦MoveNet Lightningã§å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°ã‚’å®šç¾©ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20e9467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pose_estimation(image):\n",
    "    \"\"\"\n",
    "    å˜ä¸€ç”»åƒã«å¯¾ã—ã¦å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°\n",
    "    \"\"\"\n",
    "    # ç”»åƒã‚’192x192ã«ãƒªã‚µã‚¤ã‚ºï¼ˆMoveNet Lightningã®å…¥åŠ›ã‚µã‚¤ã‚ºï¼‰\n",
    "    input_image = tf.expand_dims(image, axis=0)\n",
    "    input_image = tf.cast(tf.image.resize_with_pad(input_image, 192, 192), dtype=tf.int32)\n",
    "    \n",
    "    # å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œ\n",
    "    outputs = movenet_fn(input_image)\n",
    "    keypoints = outputs['output_0'].numpy()\n",
    "    \n",
    "    # çµæœã‚’è¿”ã™ (17å€‹ã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ Ã— [y, x, confidence])\n",
    "    return keypoints[0, 0, :, :]\n",
    "\n",
    "def analyze_pose_results(keypoints, confidence_threshold=0.3):\n",
    "    \"\"\"\n",
    "    å§¿å‹¢æ¨å®šçµæœã‚’åˆ†æã—ã€æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®æƒ…å ±ã‚’è¡¨ç¤º\n",
    "    \"\"\"\n",
    "    detected_points = []\n",
    "    \n",
    "    for name, idx in KEYPOINT_DICT.items():\n",
    "        y, x, conf = keypoints[idx]\n",
    "        if conf > confidence_threshold:\n",
    "            detected_points.append((name, conf))\n",
    "    \n",
    "    print(f\"ğŸ¯ æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ•°: {len(detected_points)}/17\")\n",
    "    print(\"ğŸ“Š æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ:\")\n",
    "    \n",
    "    for name, conf in sorted(detected_points, key=lambda x: x[1], reverse=True):\n",
    "        print(f\"   - {name}: {conf:.3f}\")\n",
    "    \n",
    "    return detected_points\n",
    "\n",
    "print(\"âœ… å§¿å‹¢æ¨å®šå®Ÿè¡Œé–¢æ•°ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f5b905",
   "metadata": {},
   "source": [
    "## 6. ãƒ†ã‚¹ãƒˆç”»åƒã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¨æº–å‚™\n",
    "\n",
    "Google Colabã«ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3eaa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "print(\"ğŸ“ å§¿å‹¢æ¨å®šã‚’è¡Œã„ãŸã„ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„...\")\n",
    "print(\"ğŸ’¡ äººç‰©ãŒå†™ã£ã¦ã„ã‚‹ç”»åƒã‚’é¸æŠã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒã®ãƒ‘ã‚¹ã‚’å–å¾—\n",
    "uploaded_image_paths = list(uploaded.keys())\n",
    "print(f\"âœ… {len(uploaded_image_paths)}å€‹ã®ç”»åƒãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸ\")\n",
    "\n",
    "# ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒã‚’è¡¨ç¤º\n",
    "if uploaded_image_paths:\n",
    "    fig, axes = plt.subplots(1, min(3, len(uploaded_image_paths)), figsize=(15, 5))\n",
    "    if len(uploaded_image_paths) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, image_path in enumerate(uploaded_image_paths[:3]):\n",
    "        img = Image.open(image_path)\n",
    "        if len(uploaded_image_paths) > 1:\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].set_title(f'ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”»åƒ {i+1}: {image_path}')\n",
    "            axes[i].axis('off')\n",
    "        else:\n",
    "            axes[0].imshow(img)\n",
    "            axes[0].set_title(f'ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”»åƒ: {image_path}')\n",
    "            axes[0].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âŒ ç”»åƒãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0c0120",
   "metadata": {},
   "source": [
    "## 7. å§¿å‹¢æ¨å®šã®å®Ÿè¡Œ\n",
    "\n",
    "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸç”»åƒã«å¯¾ã—ã¦MoveNet Lightningã«ã‚ˆã‚‹å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578bc152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œ\n",
    "if uploaded_image_paths:\n",
    "    results_list = []\n",
    "    \n",
    "    for image_path in uploaded_image_paths:\n",
    "        print(f\"ğŸ” {image_path} ã®å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œä¸­...\")\n",
    "        \n",
    "        # ç”»åƒã‚’èª­ã¿è¾¼ã¿\n",
    "        image = cv2.imread(image_path)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œ\n",
    "        keypoints = run_pose_estimation(image_rgb)\n",
    "        results_list.append((image_rgb, keypoints))\n",
    "        \n",
    "        print(f\"âœ… å§¿å‹¢æ¨å®šå®Œäº†: {image_path}\")\n",
    "        \n",
    "        # çµæœã‚’åˆ†æ\n",
    "        detected_points = analyze_pose_results(keypoints)\n",
    "        print()\n",
    "    \n",
    "    print(\"ğŸ‰ å…¨ã¦ã®ç”»åƒã®å§¿å‹¢æ¨å®šãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
    "else:\n",
    "    print(\"âŒ å§¿å‹¢æ¨å®šã™ã‚‹ç”»åƒãŒã‚ã‚Šã¾ã›ã‚“ã€‚ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a99317",
   "metadata": {},
   "source": [
    "## 8. å§¿å‹¢æ¨å®šçµæœã®å¯è¦–åŒ–\n",
    "\n",
    "æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã¨ã‚¹ã‚±ãƒ«ãƒˆãƒ³ã‚’å…ƒã®ç”»åƒã«é‡ã­ã¦è¡¨ç¤ºã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1148847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å§¿å‹¢æ¨å®šçµæœã‚’å¯è¦–åŒ–\n",
    "if uploaded_image_paths and results_list:\n",
    "    \n",
    "    # ç”»åƒæ•°ã«å¿œã˜ã¦subplotã‚’èª¿æ•´\n",
    "    num_images = len(uploaded_image_paths)\n",
    "    cols = min(2, num_images)\n",
    "    rows = (num_images + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows * 2, cols, figsize=(15, 8 * rows))\n",
    "    if num_images == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    elif rows == 1:\n",
    "        axes = axes.reshape(2, -1)\n",
    "    \n",
    "    for i, (image_path, (image_rgb, keypoints)) in enumerate(zip(uploaded_image_paths, results_list)):\n",
    "        col = i % cols\n",
    "        \n",
    "        # å…ƒç”»åƒã‚’è¡¨ç¤º\n",
    "        axes[0, col].imshow(image_rgb)\n",
    "        axes[0, col].set_title(f'å…ƒç”»åƒ: {image_path}')\n",
    "        axes[0, col].axis('off')\n",
    "        \n",
    "        # å§¿å‹¢æ¨å®šçµæœã‚’é‡ã­ãŸç”»åƒã‚’è¡¨ç¤º\n",
    "        annotated_image = draw_keypoints_and_skeleton(image_rgb, keypoints)\n",
    "        axes[1, col].imshow(annotated_image)\n",
    "        axes[1, col].set_title(f'å§¿å‹¢æ¨å®šçµæœ: {image_path}')\n",
    "        axes[1, col].axis('off')\n",
    "    \n",
    "    # ä½™ã£ãŸsubplotãŒã‚ã‚Œã°éè¡¨ç¤ºã«ã™ã‚‹\n",
    "    for i in range(num_images, cols):\n",
    "        if i < cols:\n",
    "            axes[0, i].axis('off')\n",
    "            axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"ğŸ¨ å§¿å‹¢æ¨å®šçµæœã®å¯è¦–åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
    "    print(\"ğŸ“Š è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹è¦ç´ ã®èª¬æ˜:\")\n",
    "    print(\"   - ç·‘è‰²ã®å††: æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ\")\n",
    "    print(\"   - èµ¤è‰²ã®å††: ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®å¢ƒç•Œ\")\n",
    "    print(\"   - é’è‰²ã®ç·š: ã‚¹ã‚±ãƒ«ãƒˆãƒ³ï¼ˆéª¨æ ¼ï¼‰\")\n",
    "    print(\"   - ä¿¡é ¼åº¦ãŒä½ã„ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã¯è¡¨ç¤ºã•ã‚Œã¾ã›ã‚“\")\n",
    "else:\n",
    "    print(\"âŒ è¡¨ç¤ºã™ã‚‹çµæœãŒã‚ã‚Šã¾ã›ã‚“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c003292",
   "metadata": {},
   "source": [
    "## 9. è©³ç´°åˆ†æï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "\n",
    "æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®è©³ç´°æƒ…å ±ã‚’ç¢ºèªã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae344a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è©³ç´°åˆ†æ\n",
    "if uploaded_image_paths and results_list:\n",
    "    print(\"ğŸ“‹ è©³ç´°åˆ†æçµæœ\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, (image_path, (image_rgb, keypoints)) in enumerate(zip(uploaded_image_paths, results_list)):\n",
    "        print(f\"\\nğŸ–¼ï¸  ç”»åƒ {i+1}: {image_path}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # å„ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®è©³ç´°æƒ…å ±\n",
    "        print(\"ğŸ“ å…¨ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®è©³ç´°:\")\n",
    "        for name, idx in KEYPOINT_DICT.items():\n",
    "            y, x, conf = keypoints[idx]\n",
    "            status = \"âœ… æ¤œå‡º\" if conf > 0.3 else \"âŒ æœªæ¤œå‡º\"\n",
    "            print(f\"   {name:15}: {status} (ä¿¡é ¼åº¦: {conf:.3f}, åº§æ¨™: ({x:.3f}, {y:.3f}))\")\n",
    "        \n",
    "        # æ¤œå‡ºç‡ã®è¨ˆç®—\n",
    "        detected_count = sum(1 for _, _, conf in keypoints if conf > 0.3)\n",
    "        detection_rate = detected_count / 17 * 100\n",
    "        print(f\"\\nğŸ“Š æ¤œå‡ºç‡: {detection_rate:.1f}% ({detected_count}/17 ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ)\")\n",
    "        \n",
    "        # å¹³å‡ä¿¡é ¼åº¦\n",
    "        avg_confidence = np.mean([conf for _, _, conf in keypoints if conf > 0.3])\n",
    "        print(f\"ğŸ“ˆ å¹³å‡ä¿¡é ¼åº¦: {avg_confidence:.3f}\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(\"âœ… è©³ç´°åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
    "else:\n",
    "    print(\"âŒ åˆ†æã™ã‚‹çµæœãŒã‚ã‚Šã¾ã›ã‚“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0174aa",
   "metadata": {},
   "source": [
    "## ğŸ¯ ä½¿ç”¨æ–¹æ³•ã®ã¾ã¨ã‚\n",
    "\n",
    "1. **ã‚»ãƒ«ã‚’é †ç•ªã«å®Ÿè¡Œ**: ä¸Šã‹ã‚‰é †ç•ªã«ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\n",
    "2. **ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**: ã‚»ãƒ«6ã§äººç‰©ãŒå†™ã£ãŸç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\n",
    "3. **çµæœã‚’ç¢ºèª**: ã‚»ãƒ«8ã§å§¿å‹¢æ¨å®šçµæœã‚’å¯è¦–åŒ–ã—ã¦ç¢ºèªã§ãã¾ã™\n",
    "4. **è©³ç´°åˆ†æ**: ã‚»ãƒ«9ã§å„ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®è©³ç´°æƒ…å ±ã‚’ç¢ºèªã§ãã¾ã™\n",
    "\n",
    "## ğŸ“ æ³¨æ„äº‹é …\n",
    "\n",
    "- **æœ€é©ãªç”»åƒ**: äººç‰©ãŒæ˜ç¢ºã«å†™ã£ã¦ã„ã‚‹ç”»åƒã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™\n",
    "- **ä¿¡é ¼åº¦é–¾å€¤**: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯0.3ã§ã™ãŒã€å¿…è¦ã«å¿œã˜ã¦èª¿æ•´ã§ãã¾ã™\n",
    "- **ãƒ¢ãƒ‡ãƒ«ã®åˆ¶é™**: å˜ä¸€äººç‰©ã®å§¿å‹¢æ¨å®šã«ã®ã¿å¯¾å¿œã—ã¦ã„ã¾ã™\n",
    "- **GPUåˆ©ç”¨**: Google Colabã§GPUãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã‚ˆã‚Šé«˜é€Ÿã«å‹•ä½œã—ã¾ã™\n",
    "\n",
    "## ğŸ”— å‚è€ƒãƒªãƒ³ã‚¯\n",
    "\n",
    "- [MoveNet: Ultra fast and accurate pose detection model](https://blog.tensorflow.org/2021/05/next-generation-pose-detection-with-movenet-and-tensorflowjs.html)\n",
    "- [TensorFlow Hub - MoveNet](https://tfhub.dev/google/movenet/singlepose/lightning/4)\n",
    "- [TensorFlow Pose Estimation Guide](https://www.tensorflow.org/lite/examples/pose_estimation/overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3554ffa7",
   "metadata": {},
   "source": [
    "# MoveNet Lightningã«ã‚ˆã‚‹å§¿å‹¢æ¨å®š - Google Colabç‰ˆ\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€TensorFlow Hubã®MoveNet Lightningãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å§¿å‹¢æ¨å®šã‚’è¡Œã„ã¾ã™ã€‚MoveNet Lightningã¯è»½é‡ã§é«˜é€Ÿãªå§¿å‹¢æ¨å®šãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚\n",
    "\n",
    "## ç‰¹å¾´\n",
    "- é«˜é€Ÿãªãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å§¿å‹¢æ¨å®š\n",
    "- 17å€‹ã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®æ¤œå‡º\n",
    "- è»½é‡ãƒ¢ãƒ‡ãƒ«ï¼ˆLightningç‰ˆï¼‰\n",
    "- ç”»åƒãƒ»å‹•ç”»ä¸¡æ–¹ã«å¯¾å¿œ\n",
    "- ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ä»˜ãã®çµæœ\n",
    "\n",
    "## å¿…è¦ãªç’°å¢ƒ\n",
    "- Google Colabï¼ˆæ¨å¥¨ï¼‰\n",
    "- Python 3.8ä»¥ä¸Š\n",
    "- TensorFlow 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026f3a6e",
   "metadata": {},
   "source": [
    "## 1. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "\n",
    "MoveNet Lightningã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã«å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6ae8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlowã¨TensorFlow Hubã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!pip install tensorflow tensorflow-hub\n",
    "\n",
    "# ãã®ä»–ã®å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!pip install opencv-python-headless pillow matplotlib numpy\n",
    "\n",
    "# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†ã®ç¢ºèª\n",
    "print(\"âœ… å…¨ã¦ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ec1aca",
   "metadata": {},
   "source": [
    "## 2. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨è¨­å®š\n",
    "\n",
    "å§¿å‹¢æ¨å®šã«å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada36c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "from google.colab import files\n",
    "import io\n",
    "\n",
    "# GPUåˆ©ç”¨å¯èƒ½æ€§ã®ç¢ºèª\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(\"âœ… å…¨ã¦ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4839fe8f",
   "metadata": {},
   "source": [
    "## 3. MoveNet Lightningãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "\n",
    "TensorFlow Hubã‹ã‚‰MoveNet Lightningãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14430130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MoveNet Lightning ãƒ¢ãƒ‡ãƒ«ã‚’TensorFlow Hubã‹ã‚‰èª­ã¿è¾¼ã¿\n",
    "model_url = \"https://tfhub.dev/google/movenet/singlepose/lightning/4\"\n",
    "print(\"ğŸ”„ MoveNet Lightningãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "movenet = hub.load(model_url)\n",
    "movenet_fn = movenet.signatures['serving_default']\n",
    "\n",
    "print(\"âœ… MoveNet Lightningãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
    "print(\"ğŸ“Š ãƒ¢ãƒ‡ãƒ«æƒ…å ±:\")\n",
    "print(\"   - å…¥åŠ›ã‚µã‚¤ã‚º: 192x192\")\n",
    "print(\"   - æ¤œå‡ºã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ•°: 17å€‹\")\n",
    "print(\"   - å‡ºåŠ›å½¢å¼: [y, x, confidence] Ã— 17\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509761f1",
   "metadata": {},
   "source": [
    "## 4. ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆå®šç¾©ã¨æç”»é–¢æ•°\n",
    "\n",
    "MoveNetãŒæ¤œå‡ºã™ã‚‹17å€‹ã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®å®šç¾©ã¨ã€çµæœã‚’å¯è¦–åŒ–ã™ã‚‹ãŸã‚ã®é–¢æ•°ã‚’å®šç¾©ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db8b9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MoveNetã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆå®šç¾©ï¼ˆ17å€‹ï¼‰\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "# ã‚¹ã‚±ãƒ«ãƒˆãƒ³ã®æ¥ç¶šé–¢ä¿‚\n",
    "CONNECTIONS = [\n",
    "    # é¡”\n",
    "    (0, 1), (0, 2), (1, 3), (2, 4),\n",
    "    # èƒ´ä½“\n",
    "    (5, 6), (5, 11), (6, 12), (11, 12),\n",
    "    # å·¦è…•\n",
    "    (5, 7), (7, 9),\n",
    "    # å³è…•\n",
    "    (6, 8), (8, 10),\n",
    "    # å·¦è„š\n",
    "    (11, 13), (13, 15),\n",
    "    # å³è„š\n",
    "    (12, 14), (14, 16)\n",
    "]\n",
    "\n",
    "def draw_keypoints_and_skeleton(image, keypoints, confidence_threshold=0.3):\n",
    "    \"\"\"\n",
    "    ç”»åƒã«ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã¨ã‚¹ã‚±ãƒ«ãƒˆãƒ³ã‚’æç”»ã™ã‚‹é–¢æ•°\n",
    "    \"\"\"\n",
    "    image_copy = image.copy()\n",
    "    height, width = image_copy.shape[:2]\n",
    "    \n",
    "    # ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã‚’æç”»\n",
    "    for i, (y, x, conf) in enumerate(keypoints):\n",
    "        if conf > confidence_threshold:\n",
    "            # åº§æ¨™ã‚’ç”»åƒã‚µã‚¤ã‚ºã«ã‚¹ã‚±ãƒ¼ãƒ«\n",
    "            x_coord = int(x * width)\n",
    "            y_coord = int(y * height)\n",
    "            \n",
    "            # ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã‚’å††ã§æç”»\n",
    "            cv2.circle(image_copy, (x_coord, y_coord), 5, (0, 255, 0), -1)\n",
    "            cv2.circle(image_copy, (x_coord, y_coord), 7, (0, 0, 255), 2)\n",
    "    \n",
    "    # ã‚¹ã‚±ãƒ«ãƒˆãƒ³ã‚’æç”»\n",
    "    for connection in CONNECTIONS:\n",
    "        point1_idx, point2_idx = connection\n",
    "        \n",
    "        y1, x1, conf1 = keypoints[point1_idx]\n",
    "        y2, x2, conf2 = keypoints[point2_idx]\n",
    "        \n",
    "        if conf1 > confidence_threshold and conf2 > confidence_threshold:\n",
    "            x1_coord = int(x1 * width)\n",
    "            y1_coord = int(y1 * height)\n",
    "            x2_coord = int(x2 * width)\n",
    "            y2_coord = int(y2 * height)\n",
    "            \n",
    "            cv2.line(image_copy, (x1_coord, y1_coord), (x2_coord, y2_coord), (255, 0, 0), 2)\n",
    "    \n",
    "    return image_copy\n",
    "\n",
    "print(\"âœ… ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆå®šç¾©ã¨æç”»é–¢æ•°ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61829741",
   "metadata": {},
   "source": [
    "## 5. å§¿å‹¢æ¨å®šå®Ÿè¡Œé–¢æ•°\n",
    "\n",
    "ç”»åƒã«å¯¾ã—ã¦MoveNet Lightningã§å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°ã‚’å®šç¾©ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafd995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pose_estimation(image):\n",
    "    \"\"\"\n",
    "    å˜ä¸€ç”»åƒã«å¯¾ã—ã¦å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°\n",
    "    \"\"\"\n",
    "    # ç”»åƒã‚’192x192ã«ãƒªã‚µã‚¤ã‚ºï¼ˆMoveNet Lightningã®å…¥åŠ›ã‚µã‚¤ã‚ºï¼‰\n",
    "    input_image = tf.expand_dims(image, axis=0)\n",
    "    input_image = tf.cast(tf.image.resize_with_pad(input_image, 192, 192), dtype=tf.int32)\n",
    "    \n",
    "    # å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œ\n",
    "    outputs = movenet_fn(input_image)\n",
    "    keypoints = outputs['output_0'].numpy()\n",
    "    \n",
    "    # çµæœã‚’è¿”ã™ (17å€‹ã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ Ã— [y, x, confidence])\n",
    "    return keypoints[0, 0, :, :]\n",
    "\n",
    "def analyze_pose_results(keypoints, confidence_threshold=0.3):\n",
    "    \"\"\"\n",
    "    å§¿å‹¢æ¨å®šçµæœã‚’åˆ†æã—ã€æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®æƒ…å ±ã‚’è¡¨ç¤º\n",
    "    \"\"\"\n",
    "    detected_points = []\n",
    "    \n",
    "    for name, idx in KEYPOINT_DICT.items():\n",
    "        y, x, conf = keypoints[idx]\n",
    "        if conf > confidence_threshold:\n",
    "            detected_points.append((name, conf))\n",
    "    \n",
    "    print(f\"ğŸ¯ æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ•°: {len(detected_points)}/17\")\n",
    "    print(\"ğŸ“Š æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ:\")\n",
    "    \n",
    "    for name, conf in sorted(detected_points, key=lambda x: x[1], reverse=True):\n",
    "        print(f\"   - {name}: {conf:.3f}\")\n",
    "    \n",
    "    return detected_points\n",
    "\n",
    "print(\"âœ… å§¿å‹¢æ¨å®šå®Ÿè¡Œé–¢æ•°ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607e9cc5",
   "metadata": {},
   "source": [
    "## 6. ãƒ†ã‚¹ãƒˆç”»åƒã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¨æº–å‚™\n",
    "\n",
    "Google Colabã«ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef2d240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "print(\"ğŸ“ å§¿å‹¢æ¨å®šã‚’è¡Œã„ãŸã„ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„...\")\n",
    "print(\"ğŸ’¡ äººç‰©ãŒå†™ã£ã¦ã„ã‚‹ç”»åƒã‚’é¸æŠã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒã®ãƒ‘ã‚¹ã‚’å–å¾—\n",
    "uploaded_image_paths = list(uploaded.keys())\n",
    "print(f\"âœ… {len(uploaded_image_paths)}å€‹ã®ç”»åƒãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸ\")\n",
    "\n",
    "# ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒã‚’è¡¨ç¤º\n",
    "if uploaded_image_paths:\n",
    "    fig, axes = plt.subplots(1, min(3, len(uploaded_image_paths)), figsize=(15, 5))\n",
    "    if len(uploaded_image_paths) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, image_path in enumerate(uploaded_image_paths[:3]):\n",
    "        img = Image.open(image_path)\n",
    "        if len(uploaded_image_paths) > 1:\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].set_title(f'ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”»åƒ {i+1}: {image_path}')\n",
    "            axes[i].axis('off')\n",
    "        else:\n",
    "            axes[0].imshow(img)\n",
    "            axes[0].set_title(f'ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”»åƒ: {image_path}')\n",
    "            axes[0].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âŒ ç”»åƒãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83800ca",
   "metadata": {},
   "source": [
    "## 7. å§¿å‹¢æ¨å®šã®å®Ÿè¡Œ\n",
    "\n",
    "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸç”»åƒã«å¯¾ã—ã¦MoveNet Lightningã«ã‚ˆã‚‹å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e722dce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œ\n",
    "if uploaded_image_paths:\n",
    "    results_list = []\n",
    "    \n",
    "    for image_path in uploaded_image_paths:\n",
    "        print(f\"ğŸ” {image_path} ã®å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œä¸­...\")\n",
    "        \n",
    "        # ç”»åƒã‚’èª­ã¿è¾¼ã¿\n",
    "        image = cv2.imread(image_path)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œ\n",
    "        keypoints = run_pose_estimation(image_rgb)\n",
    "        results_list.append((image_rgb, keypoints))\n",
    "        \n",
    "        print(f\"âœ… å§¿å‹¢æ¨å®šå®Œäº†: {image_path}\")\n",
    "        \n",
    "        # çµæœã‚’åˆ†æ\n",
    "        detected_points = analyze_pose_results(keypoints)\n",
    "        print()\n",
    "    \n",
    "    print(\"ğŸ‰ å…¨ã¦ã®ç”»åƒã®å§¿å‹¢æ¨å®šãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
    "else:\n",
    "    print(\"âŒ å§¿å‹¢æ¨å®šã™ã‚‹ç”»åƒãŒã‚ã‚Šã¾ã›ã‚“ã€‚ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60218de2",
   "metadata": {},
   "source": [
    "## 8. å§¿å‹¢æ¨å®šçµæœã®å¯è¦–åŒ–\n",
    "\n",
    "æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã¨ã‚¹ã‚±ãƒ«ãƒˆãƒ³ã‚’å…ƒã®ç”»åƒã«é‡ã­ã¦è¡¨ç¤ºã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7608ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å§¿å‹¢æ¨å®šçµæœã‚’å¯è¦–åŒ–\n",
    "if uploaded_image_paths and results_list:\n",
    "    \n",
    "    # ç”»åƒæ•°ã«å¿œã˜ã¦subplotã‚’èª¿æ•´\n",
    "    num_images = len(uploaded_image_paths)\n",
    "    cols = min(2, num_images)\n",
    "    rows = (num_images + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows * 2, cols, figsize=(15, 8 * rows))\n",
    "    if num_images == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    elif rows == 1:\n",
    "        axes = axes.reshape(2, -1)\n",
    "    \n",
    "    for i, (image_path, (image_rgb, keypoints)) in enumerate(zip(uploaded_image_paths, results_list)):\n",
    "        col = i % cols\n",
    "        \n",
    "        # å…ƒç”»åƒã‚’è¡¨ç¤º\n",
    "        axes[0, col].imshow(image_rgb)\n",
    "        axes[0, col].set_title(f'å…ƒç”»åƒ: {image_path}')\n",
    "        axes[0, col].axis('off')\n",
    "        \n",
    "        # å§¿å‹¢æ¨å®šçµæœã‚’é‡ã­ãŸç”»åƒã‚’è¡¨ç¤º\n",
    "        annotated_image = draw_keypoints_and_skeleton(image_rgb, keypoints)\n",
    "        axes[1, col].imshow(annotated_image)\n",
    "        axes[1, col].set_title(f'å§¿å‹¢æ¨å®šçµæœ: {image_path}')\n",
    "        axes[1, col].axis('off')\n",
    "    \n",
    "    # ä½™ã£ãŸsubplotãŒã‚ã‚Œã°éè¡¨ç¤ºã«ã™ã‚‹\n",
    "    for i in range(num_images, cols):\n",
    "        if i < cols:\n",
    "            axes[0, i].axis('off')\n",
    "            axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"ğŸ¨ å§¿å‹¢æ¨å®šçµæœã®å¯è¦–åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
    "    print(\"ğŸ“Š è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹è¦ç´ ã®èª¬æ˜:\")\n",
    "    print(\"   - ç·‘è‰²ã®å††: æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ\")\n",
    "    print(\"   - èµ¤è‰²ã®å††: ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®å¢ƒç•Œ\")\n",
    "    print(\"   - é’è‰²ã®ç·š: ã‚¹ã‚±ãƒ«ãƒˆãƒ³ï¼ˆéª¨æ ¼ï¼‰\")\n",
    "    print(\"   - ä¿¡é ¼åº¦ãŒä½ã„ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã¯è¡¨ç¤ºã•ã‚Œã¾ã›ã‚“\")\n",
    "else:\n",
    "    print(\"âŒ è¡¨ç¤ºã™ã‚‹çµæœãŒã‚ã‚Šã¾ã›ã‚“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe272958",
   "metadata": {},
   "source": [
    "## 9. è©³ç´°åˆ†æï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "\n",
    "æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®è©³ç´°æƒ…å ±ã‚’ç¢ºèªã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64189272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è©³ç´°åˆ†æ\n",
    "if uploaded_image_paths and results_list:\n",
    "    print(\"ğŸ“‹ è©³ç´°åˆ†æçµæœ\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, (image_path, (image_rgb, keypoints)) in enumerate(zip(uploaded_image_paths, results_list)):\n",
    "        print(f\"\\nğŸ–¼ï¸  ç”»åƒ {i+1}: {image_path}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # å„ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®è©³ç´°æƒ…å ±\n",
    "        print(\"ğŸ“ å…¨ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®è©³ç´°:\")\n",
    "        for name, idx in KEYPOINT_DICT.items():\n",
    "            y, x, conf = keypoints[idx]\n",
    "            status = \"âœ… æ¤œå‡º\" if conf > 0.3 else \"âŒ æœªæ¤œå‡º\"\n",
    "            print(f\"   {name:15}: {status} (ä¿¡é ¼åº¦: {conf:.3f}, åº§æ¨™: ({x:.3f}, {y:.3f}))\")\n",
    "        \n",
    "        # æ¤œå‡ºç‡ã®è¨ˆç®—\n",
    "        detected_count = sum(1 for _, _, conf in keypoints if conf > 0.3)\n",
    "        detection_rate = detected_count / 17 * 100\n",
    "        print(f\"\\nğŸ“Š æ¤œå‡ºç‡: {detection_rate:.1f}% ({detected_count}/17 ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ)\")\n",
    "        \n",
    "        # å¹³å‡ä¿¡é ¼åº¦\n",
    "        avg_confidence = np.mean([conf for _, _, conf in keypoints if conf > 0.3])\n",
    "        print(f\"ğŸ“ˆ å¹³å‡ä¿¡é ¼åº¦: {avg_confidence:.3f}\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(\"âœ… è©³ç´°åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
    "else:\n",
    "    print(\"âŒ åˆ†æã™ã‚‹çµæœãŒã‚ã‚Šã¾ã›ã‚“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d806b74",
   "metadata": {},
   "source": [
    "## ğŸ¯ ä½¿ç”¨æ–¹æ³•ã®ã¾ã¨ã‚\n",
    "\n",
    "1. **é †ç•ªã«å®Ÿè¡Œ**: ä¸Šã‹ã‚‰é †ç•ªã«ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\n",
    "2. **ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**: 6ç•ªç›®ã®ã‚»ãƒ«ã§äººç‰©ãŒå†™ã£ãŸç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\n",
    "3. **çµæœã‚’ç¢ºèª**: 8ç•ªç›®ã®ã‚»ãƒ«ã§å§¿å‹¢æ¨å®šçµæœã‚’å¯è¦–åŒ–ã—ã¦ç¢ºèªã§ãã¾ã™\n",
    "4. **è©³ç´°åˆ†æ**: 9ç•ªç›®ã®ã‚»ãƒ«ã§å„ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®è©³ç´°æƒ…å ±ã‚’ç¢ºèªã§ãã¾ã™\n",
    "\n",
    "## ğŸ“ æ³¨æ„äº‹é …\n",
    "\n",
    "- **æœ€é©ãªç”»åƒ**: äººç‰©ãŒæ˜ç¢ºã«å†™ã£ã¦ã„ã‚‹ç”»åƒã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™\n",
    "- **ä¿¡é ¼åº¦é–¾å€¤**: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯0.3ã§ã™ãŒã€å¿…è¦ã«å¿œã˜ã¦èª¿æ•´ã§ãã¾ã™\n",
    "- **ãƒ¢ãƒ‡ãƒ«ã®åˆ¶é™**: å˜ä¸€äººç‰©ã®å§¿å‹¢æ¨å®šã«ã®ã¿å¯¾å¿œã—ã¦ã„ã¾ã™\n",
    "- **GPUåˆ©ç”¨**: Google Colabã§GPUãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã‚ˆã‚Šé«˜é€Ÿã«å‹•ä½œã—ã¾ã™\n",
    "\n",
    "## ğŸ”— å‚è€ƒãƒªãƒ³ã‚¯\n",
    "\n",
    "- [MoveNet: Ultra fast and accurate pose detection model](https://blog.tensorflow.org/2021/05/next-generation-pose-detection-with-movenet-and-tensorflowjs.html)\n",
    "- [TensorFlow Hub - MoveNet](https://tfhub.dev/google/movenet/singlepose/lightning/4)\n",
    "- [TensorFlow Pose Estimation Guide](https://www.tensorflow.org/lite/examples/pose_estimation/overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86bb82b",
   "metadata": {},
   "source": [
    "# MoveNet Lightningã«ã‚ˆã‚‹å§¿å‹¢æ¨å®š - Google Colabç‰ˆ\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€TensorFlow Hubã®MoveNet Lightningãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å§¿å‹¢æ¨å®šã‚’è¡Œã„ã¾ã™ã€‚MoveNet Lightningã¯è»½é‡ã§é«˜é€Ÿãªå§¿å‹¢æ¨å®šãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚\n",
    "\n",
    "## ç‰¹å¾´\n",
    "- é«˜é€Ÿãªãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å§¿å‹¢æ¨å®š\n",
    "- 17å€‹ã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®æ¤œå‡º\n",
    "- è»½é‡ãƒ¢ãƒ‡ãƒ«ï¼ˆLightningç‰ˆï¼‰\n",
    "- ç”»åƒãƒ»å‹•ç”»ä¸¡æ–¹ã«å¯¾å¿œ\n",
    "- ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ä»˜ãã®çµæœ\n",
    "\n",
    "## å¿…è¦ãªç’°å¢ƒ\n",
    "- Google Colabï¼ˆæ¨å¥¨ï¼‰\n",
    "- Python 3.8ä»¥ä¸Š\n",
    "- TensorFlow 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76559edc",
   "metadata": {},
   "source": [
    "## 1. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "\n",
    "MoveNet Lightningã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã«å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8420144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlowã¨TensorFlow Hubã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!pip install tensorflow tensorflow-hub\n",
    "\n",
    "# ãã®ä»–ã®å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!pip install opencv-python-headless pillow matplotlib numpy\n",
    "\n",
    "# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†ã®ç¢ºèª\n",
    "print(\"âœ… å…¨ã¦ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746f1e1a",
   "metadata": {},
   "source": [
    "## 2. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨è¨­å®š\n",
    "\n",
    "å§¿å‹¢æ¨å®šã«å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a226fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "from google.colab import files\n",
    "import io\n",
    "\n",
    "# GPUåˆ©ç”¨å¯èƒ½æ€§ã®ç¢ºèª\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(\"âœ… å…¨ã¦ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e00b3b8",
   "metadata": {},
   "source": [
    "## 3. MoveNet Lightningãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "\n",
    "TensorFlow Hubã‹ã‚‰MoveNet Lightningãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d18ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MoveNet Lightning ãƒ¢ãƒ‡ãƒ«ã‚’TensorFlow Hubã‹ã‚‰èª­ã¿è¾¼ã¿\n",
    "model_url = \"https://tfhub.dev/google/movenet/singlepose/lightning/4\"\n",
    "print(\"ğŸ”„ MoveNet Lightningãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "movenet = hub.load(model_url)\n",
    "movenet_fn = movenet.signatures['serving_default']\n",
    "\n",
    "print(\"âœ… MoveNet Lightningãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
    "print(\"ğŸ“Š ãƒ¢ãƒ‡ãƒ«æƒ…å ±:\")\n",
    "print(\"   - å…¥åŠ›ã‚µã‚¤ã‚º: 192x192\")\n",
    "print(\"   - æ¤œå‡ºã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ•°: 17å€‹\")\n",
    "print(\"   - å‡ºåŠ›å½¢å¼: [y, x, confidence] Ã— 17\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e3e842",
   "metadata": {},
   "source": [
    "## 4. ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆå®šç¾©ã¨æç”»é–¢æ•°\n",
    "\n",
    "MoveNetãŒæ¤œå‡ºã™ã‚‹17å€‹ã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®å®šç¾©ã¨ã€çµæœã‚’å¯è¦–åŒ–ã™ã‚‹ãŸã‚ã®é–¢æ•°ã‚’å®šç¾©ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7631f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MoveNetã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆå®šç¾©ï¼ˆ17å€‹ï¼‰\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "# ã‚¹ã‚±ãƒ«ãƒˆãƒ³ã®æ¥ç¶šé–¢ä¿‚\n",
    "CONNECTIONS = [\n",
    "    # é¡”\n",
    "    (0, 1), (0, 2), (1, 3), (2, 4),\n",
    "    # èƒ´ä½“\n",
    "    (5, 6), (5, 11), (6, 12), (11, 12),\n",
    "    # å·¦è…•\n",
    "    (5, 7), (7, 9),\n",
    "    # å³è…•\n",
    "    (6, 8), (8, 10),\n",
    "    # å·¦è„š\n",
    "    (11, 13), (13, 15),\n",
    "    # å³è„š\n",
    "    (12, 14), (14, 16)\n",
    "]\n",
    "\n",
    "def draw_keypoints_and_skeleton(image, keypoints, confidence_threshold=0.3):\n",
    "    \"\"\"\n",
    "    ç”»åƒã«ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã¨ã‚¹ã‚±ãƒ«ãƒˆãƒ³ã‚’æç”»ã™ã‚‹é–¢æ•°\n",
    "    \"\"\"\n",
    "    image_copy = image.copy()\n",
    "    height, width = image_copy.shape[:2]\n",
    "    \n",
    "    # ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã‚’æç”»\n",
    "    for i, (y, x, conf) in enumerate(keypoints):\n",
    "        if conf > confidence_threshold:\n",
    "            # åº§æ¨™ã‚’ç”»åƒã‚µã‚¤ã‚ºã«ã‚¹ã‚±ãƒ¼ãƒ«\n",
    "            x_coord = int(x * width)\n",
    "            y_coord = int(y * height)\n",
    "            \n",
    "            # ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã‚’å††ã§æç”»\n",
    "            cv2.circle(image_copy, (x_coord, y_coord), 5, (0, 255, 0), -1)\n",
    "            cv2.circle(image_copy, (x_coord, y_coord), 7, (0, 0, 255), 2)\n",
    "    \n",
    "    # ã‚¹ã‚±ãƒ«ãƒˆãƒ³ã‚’æç”»\n",
    "    for connection in CONNECTIONS:\n",
    "        point1_idx, point2_idx = connection\n",
    "        \n",
    "        y1, x1, conf1 = keypoints[point1_idx]\n",
    "        y2, x2, conf2 = keypoints[point2_idx]\n",
    "        \n",
    "        if conf1 > confidence_threshold and conf2 > confidence_threshold:\n",
    "            x1_coord = int(x1 * width)\n",
    "            y1_coord = int(y1 * height)\n",
    "            x2_coord = int(x2 * width)\n",
    "            y2_coord = int(y2 * height)\n",
    "            \n",
    "            cv2.line(image_copy, (x1_coord, y1_coord), (x2_coord, y2_coord), (255, 0, 0), 2)\n",
    "    \n",
    "    return image_copy\n",
    "\n",
    "print(\"âœ… ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆå®šç¾©ã¨æç”»é–¢æ•°ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5feb65",
   "metadata": {},
   "source": [
    "## 5. å§¿å‹¢æ¨å®šå®Ÿè¡Œé–¢æ•°\n",
    "\n",
    "ç”»åƒã«å¯¾ã—ã¦MoveNet Lightningã§å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°ã‚’å®šç¾©ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ce1560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pose_estimation(image):\n",
    "    \"\"\"\n",
    "    å˜ä¸€ç”»åƒã«å¯¾ã—ã¦å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°\n",
    "    \"\"\"\n",
    "    # ç”»åƒã‚’192x192ã«ãƒªã‚µã‚¤ã‚ºï¼ˆMoveNet Lightningã®å…¥åŠ›ã‚µã‚¤ã‚ºï¼‰\n",
    "    input_image = tf.expand_dims(image, axis=0)\n",
    "    input_image = tf.cast(tf.image.resize_with_pad(input_image, 192, 192), dtype=tf.int32)\n",
    "    \n",
    "    # å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œ\n",
    "    outputs = movenet_fn(input_image)\n",
    "    keypoints = outputs['output_0'].numpy()\n",
    "    \n",
    "    # çµæœã‚’è¿”ã™ (17å€‹ã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ Ã— [y, x, confidence])\n",
    "    return keypoints[0, 0, :, :]\n",
    "\n",
    "def analyze_pose_results(keypoints, confidence_threshold=0.3):\n",
    "    \"\"\"\n",
    "    å§¿å‹¢æ¨å®šçµæœã‚’åˆ†æã—ã€æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®æƒ…å ±ã‚’è¡¨ç¤º\n",
    "    \"\"\"\n",
    "    detected_points = []\n",
    "    \n",
    "    for name, idx in KEYPOINT_DICT.items():\n",
    "        y, x, conf = keypoints[idx]\n",
    "        if conf > confidence_threshold:\n",
    "            detected_points.append((name, conf))\n",
    "    \n",
    "    print(f\"ğŸ¯ æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ•°: {len(detected_points)}/17\")\n",
    "    print(\"ğŸ“Š æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ:\")\n",
    "    \n",
    "    for name, conf in sorted(detected_points, key=lambda x: x[1], reverse=True):\n",
    "        print(f\"   - {name}: {conf:.3f}\")\n",
    "    \n",
    "    return detected_points\n",
    "\n",
    "print(\"âœ… å§¿å‹¢æ¨å®šå®Ÿè¡Œé–¢æ•°ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc75301",
   "metadata": {},
   "source": [
    "## 6. ãƒ†ã‚¹ãƒˆç”»åƒã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¨æº–å‚™\n",
    "\n",
    "Google Colabã«ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e81641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "print(\"ğŸ“ å§¿å‹¢æ¨å®šã‚’è¡Œã„ãŸã„ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„...\")\n",
    "print(\"ğŸ’¡ äººç‰©ãŒå†™ã£ã¦ã„ã‚‹ç”»åƒã‚’é¸æŠã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒã®ãƒ‘ã‚¹ã‚’å–å¾—\n",
    "uploaded_image_paths = list(uploaded.keys())\n",
    "print(f\"âœ… {len(uploaded_image_paths)}å€‹ã®ç”»åƒãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸ\")\n",
    "\n",
    "# ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒã‚’è¡¨ç¤º\n",
    "if uploaded_image_paths:\n",
    "    fig, axes = plt.subplots(1, min(3, len(uploaded_image_paths)), figsize=(15, 5))\n",
    "    if len(uploaded_image_paths) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, image_path in enumerate(uploaded_image_paths[:3]):\n",
    "        img = Image.open(image_path)\n",
    "        if len(uploaded_image_paths) > 1:\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].set_title(f'ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”»åƒ {i+1}: {image_path}')\n",
    "            axes[i].axis('off')\n",
    "        else:\n",
    "            axes[0].imshow(img)\n",
    "            axes[0].set_title(f'ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”»åƒ: {image_path}')\n",
    "            axes[0].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âŒ ç”»åƒãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b1b8a7",
   "metadata": {},
   "source": [
    "## 7. å§¿å‹¢æ¨å®šã®å®Ÿè¡Œ\n",
    "\n",
    "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸç”»åƒã«å¯¾ã—ã¦MoveNet Lightningã«ã‚ˆã‚‹å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f447bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œ\n",
    "if uploaded_image_paths:\n",
    "    results_list = []\n",
    "    \n",
    "    for image_path in uploaded_image_paths:\n",
    "        print(f\"ğŸ” {image_path} ã®å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œä¸­...\")\n",
    "        \n",
    "        # ç”»åƒã‚’èª­ã¿è¾¼ã¿\n",
    "        image = cv2.imread(image_path)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œ\n",
    "        keypoints = run_pose_estimation(image_rgb)\n",
    "        results_list.append((image_rgb, keypoints))\n",
    "        \n",
    "        print(f\"âœ… å§¿å‹¢æ¨å®šå®Œäº†: {image_path}\")\n",
    "        \n",
    "        # çµæœã‚’åˆ†æ\n",
    "        detected_points = analyze_pose_results(keypoints)\n",
    "        print()\n",
    "    \n",
    "    print(\"ğŸ‰ å…¨ã¦ã®ç”»åƒã®å§¿å‹¢æ¨å®šãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
    "else:\n",
    "    print(\"âŒ å§¿å‹¢æ¨å®šã™ã‚‹ç”»åƒãŒã‚ã‚Šã¾ã›ã‚“ã€‚ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5c52e0",
   "metadata": {},
   "source": [
    "## 8. å§¿å‹¢æ¨å®šçµæœã®å¯è¦–åŒ–\n",
    "\n",
    "æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã¨ã‚¹ã‚±ãƒ«ãƒˆãƒ³ã‚’å…ƒã®ç”»åƒã«é‡ã­ã¦è¡¨ç¤ºã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3df476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å§¿å‹¢æ¨å®šçµæœã‚’å¯è¦–åŒ–\n",
    "if uploaded_image_paths and results_list:\n",
    "    \n",
    "    # ç”»åƒæ•°ã«å¿œã˜ã¦subplotã‚’èª¿æ•´\n",
    "    num_images = len(uploaded_image_paths)\n",
    "    cols = min(2, num_images)\n",
    "    rows = (num_images + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows * 2, cols, figsize=(15, 8 * rows))\n",
    "    if num_images == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    elif rows == 1:\n",
    "        axes = axes.reshape(2, -1)\n",
    "    \n",
    "    for i, (image_path, (image_rgb, keypoints)) in enumerate(zip(uploaded_image_paths, results_list)):\n",
    "        col = i % cols\n",
    "        \n",
    "        # å…ƒç”»åƒã‚’è¡¨ç¤º\n",
    "        axes[0, col].imshow(image_rgb)\n",
    "        axes[0, col].set_title(f'å…ƒç”»åƒ: {image_path}')\n",
    "        axes[0, col].axis('off')\n",
    "        \n",
    "        # å§¿å‹¢æ¨å®šçµæœã‚’é‡ã­ãŸç”»åƒã‚’è¡¨ç¤º\n",
    "        annotated_image = draw_keypoints_and_skeleton(image_rgb, keypoints)\n",
    "        axes[1, col].imshow(annotated_image)\n",
    "        axes[1, col].set_title(f'å§¿å‹¢æ¨å®šçµæœ: {image_path}')\n",
    "        axes[1, col].axis('off')\n",
    "    \n",
    "    # ä½™ã£ãŸsubplotãŒã‚ã‚Œã°éè¡¨ç¤ºã«ã™ã‚‹\n",
    "    for i in range(num_images, cols):\n",
    "        if i < cols:\n",
    "            axes[0, i].axis('off')\n",
    "            axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"ğŸ¨ å§¿å‹¢æ¨å®šçµæœã®å¯è¦–åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
    "    print(\"ğŸ“Š è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹è¦ç´ ã®èª¬æ˜:\")\n",
    "    print(\"   - ç·‘è‰²ã®å††: æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ\")\n",
    "    print(\"   - èµ¤è‰²ã®å††: ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®å¢ƒç•Œ\")\n",
    "    print(\"   - é’è‰²ã®ç·š: ã‚¹ã‚±ãƒ«ãƒˆãƒ³ï¼ˆéª¨æ ¼ï¼‰\")\n",
    "    print(\"   - ä¿¡é ¼åº¦ãŒä½ã„ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã¯è¡¨ç¤ºã•ã‚Œã¾ã›ã‚“\")\n",
    "else:\n",
    "    print(\"âŒ è¡¨ç¤ºã™ã‚‹çµæœãŒã‚ã‚Šã¾ã›ã‚“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da638b36",
   "metadata": {},
   "source": [
    "## 9. è©³ç´°åˆ†æï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "\n",
    "æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®è©³ç´°æƒ…å ±ã‚’ç¢ºèªã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e172491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è©³ç´°åˆ†æ\n",
    "if uploaded_image_paths and results_list:\n",
    "    print(\"ğŸ“‹ è©³ç´°åˆ†æçµæœ\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, (image_path, (image_rgb, keypoints)) in enumerate(zip(uploaded_image_paths, results_list)):\n",
    "        print(f\"\\nğŸ–¼ï¸  ç”»åƒ {i+1}: {image_path}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # å„ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®è©³ç´°æƒ…å ±\n",
    "        print(\"ğŸ“ å…¨ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®è©³ç´°:\")\n",
    "        for name, idx in KEYPOINT_DICT.items():\n",
    "            y, x, conf = keypoints[idx]\n",
    "            status = \"âœ… æ¤œå‡º\" if conf > 0.3 else \"âŒ æœªæ¤œå‡º\"\n",
    "            print(f\"   {name:15}: {status} (ä¿¡é ¼åº¦: {conf:.3f}, åº§æ¨™: ({x:.3f}, {y:.3f}))\")\n",
    "        \n",
    "        # æ¤œå‡ºç‡ã®è¨ˆç®—\n",
    "        detected_count = sum(1 for _, _, conf in keypoints if conf > 0.3)\n",
    "        detection_rate = detected_count / 17 * 100\n",
    "        print(f\"\\nğŸ“Š æ¤œå‡ºç‡: {detection_rate:.1f}% ({detected_count}/17 ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ)\")\n",
    "        \n",
    "        # å¹³å‡ä¿¡é ¼åº¦\n",
    "        avg_confidence = np.mean([conf for _, _, conf in keypoints if conf > 0.3])\n",
    "        print(f\"ğŸ“ˆ å¹³å‡ä¿¡é ¼åº¦: {avg_confidence:.3f}\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(\"âœ… è©³ç´°åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
    "else:\n",
    "    print(\"âŒ åˆ†æã™ã‚‹çµæœãŒã‚ã‚Šã¾ã›ã‚“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145973f8",
   "metadata": {},
   "source": [
    "## ğŸ¯ ä½¿ç”¨æ–¹æ³•ã®ã¾ã¨ã‚\n",
    "\n",
    "1. **ã‚»ãƒ«ã‚’é †ç•ªã«å®Ÿè¡Œ**: ä¸Šã‹ã‚‰é †ç•ªã«ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\n",
    "2. **ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**: 6ç•ªç›®ã®ã‚»ãƒ«ã§äººç‰©ãŒå†™ã£ãŸç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\n",
    "3. **çµæœã‚’ç¢ºèª**: 8ç•ªç›®ã®ã‚»ãƒ«ã§å§¿å‹¢æ¨å®šçµæœã‚’å¯è¦–åŒ–ã—ã¦ç¢ºèªã§ãã¾ã™\n",
    "4. **è©³ç´°åˆ†æ**: 9ç•ªç›®ã®ã‚»ãƒ«ã§å„ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®è©³ç´°æƒ…å ±ã‚’ç¢ºèªã§ãã¾ã™\n",
    "\n",
    "## ğŸ“ æ³¨æ„äº‹é …\n",
    "\n",
    "- **æœ€é©ãªç”»åƒ**: äººç‰©ãŒæ˜ç¢ºã«å†™ã£ã¦ã„ã‚‹ç”»åƒã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™\n",
    "- **ä¿¡é ¼åº¦é–¾å€¤**: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯0.3ã§ã™ãŒã€å¿…è¦ã«å¿œã˜ã¦èª¿æ•´ã§ãã¾ã™\n",
    "- **ãƒ¢ãƒ‡ãƒ«ã®åˆ¶é™**: å˜ä¸€äººç‰©ã®å§¿å‹¢æ¨å®šã«ã®ã¿å¯¾å¿œã—ã¦ã„ã¾ã™\n",
    "- **GPUåˆ©ç”¨**: Google Colabã§GPUãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã‚ˆã‚Šé«˜é€Ÿã«å‹•ä½œã—ã¾ã™\n",
    "\n",
    "## ğŸ”— å‚è€ƒãƒªãƒ³ã‚¯\n",
    "\n",
    "- [MoveNet: Ultra fast and accurate pose detection model](https://blog.tensorflow.org/2021/05/next-generation-pose-detection-with-movenet-and-tensorflowjs.html)\n",
    "- [TensorFlow Hub - MoveNet](https://tfhub.dev/google/movenet/singlepose/lightning/4)\n",
    "- [TensorFlow Pose Estimation Guide](https://www.tensorflow.org/lite/examples/pose_estimation/overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c537f906",
   "metadata": {},
   "source": [
    "# MoveNet Lightningã«ã‚ˆã‚‹å§¿å‹¢æ¨å®š - Google Colabç‰ˆ\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€TensorFlow Hubã®MoveNet Lightningãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å§¿å‹¢æ¨å®šã‚’è¡Œã„ã¾ã™ã€‚MoveNet Lightningã¯è»½é‡ã§é«˜é€Ÿãªå§¿å‹¢æ¨å®šãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚\n",
    "\n",
    "## ç‰¹å¾´\n",
    "- é«˜é€Ÿãªãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å§¿å‹¢æ¨å®š\n",
    "- 17å€‹ã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®æ¤œå‡º\n",
    "- è»½é‡ãƒ¢ãƒ‡ãƒ«ï¼ˆLightningç‰ˆï¼‰\n",
    "- ç”»åƒãƒ»å‹•ç”»ä¸¡æ–¹ã«å¯¾å¿œ\n",
    "- ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ä»˜ãã®çµæœ\n",
    "\n",
    "## å¿…è¦ãªç’°å¢ƒ\n",
    "- Google Colabï¼ˆæ¨å¥¨ï¼‰\n",
    "- Python 3.8ä»¥ä¸Š\n",
    "- TensorFlow 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1df3fc9",
   "metadata": {},
   "source": [
    "## 1. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "\n",
    "MoveNet Lightningã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã«å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c23f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlowã¨TensorFlow Hubã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!pip install tensorflow tensorflow-hub\n",
    "\n",
    "# ãã®ä»–ã®å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!pip install opencv-python-headless pillow matplotlib numpy\n",
    "\n",
    "# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†ã®ç¢ºèª\n",
    "print(\"âœ… å…¨ã¦ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58d767a",
   "metadata": {},
   "source": [
    "## 2. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨è¨­å®š\n",
    "\n",
    "å§¿å‹¢æ¨å®šã«å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7150e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "from google.colab import files\n",
    "import io\n",
    "\n",
    "# GPUåˆ©ç”¨å¯èƒ½æ€§ã®ç¢ºèª\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(\"âœ… å…¨ã¦ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b5d529",
   "metadata": {},
   "source": [
    "## 3. MoveNet Lightningãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "\n",
    "TensorFlow Hubã‹ã‚‰MoveNet Lightningãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abe4ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MoveNet Lightning ãƒ¢ãƒ‡ãƒ«ã‚’TensorFlow Hubã‹ã‚‰èª­ã¿è¾¼ã¿\n",
    "model_url = \"https://tfhub.dev/google/movenet/singlepose/lightning/4\"\n",
    "print(\"ğŸ”„ MoveNet Lightningãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "movenet = hub.load(model_url)\n",
    "movenet_fn = movenet.signatures['serving_default']\n",
    "\n",
    "print(\"âœ… MoveNet Lightningãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
    "print(\"ğŸ“Š ãƒ¢ãƒ‡ãƒ«æƒ…å ±:\")\n",
    "print(\"   - å…¥åŠ›ã‚µã‚¤ã‚º: 192x192\")\n",
    "print(\"   - æ¤œå‡ºã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ•°: 17å€‹\")\n",
    "print(\"   - å‡ºåŠ›å½¢å¼: [y, x, confidence] Ã— 17\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dbabe7",
   "metadata": {},
   "source": [
    "## 4. ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆå®šç¾©ã¨æç”»é–¢æ•°\n",
    "\n",
    "MoveNetãŒæ¤œå‡ºã™ã‚‹17å€‹ã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®å®šç¾©ã¨ã€çµæœã‚’å¯è¦–åŒ–ã™ã‚‹ãŸã‚ã®é–¢æ•°ã‚’å®šç¾©ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcf83f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MoveNetã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆå®šç¾©ï¼ˆ17å€‹ï¼‰\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "# ã‚¹ã‚±ãƒ«ãƒˆãƒ³ã®æ¥ç¶šé–¢ä¿‚\n",
    "CONNECTIONS = [\n",
    "    # é¡”\n",
    "    (0, 1), (0, 2), (1, 3), (2, 4),\n",
    "    # èƒ´ä½“\n",
    "    (5, 6), (5, 11), (6, 12), (11, 12),\n",
    "    # å·¦è…•\n",
    "    (5, 7), (7, 9),\n",
    "    # å³è…•\n",
    "    (6, 8), (8, 10),\n",
    "    # å·¦è„š\n",
    "    (11, 13), (13, 15),\n",
    "    # å³è„š\n",
    "    (12, 14), (14, 16)\n",
    "]\n",
    "\n",
    "def draw_keypoints_and_skeleton(image, keypoints, confidence_threshold=0.3):\n",
    "    \"\"\"\n",
    "    ç”»åƒã«ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã¨ã‚¹ã‚±ãƒ«ãƒˆãƒ³ã‚’æç”»ã™ã‚‹é–¢æ•°\n",
    "    \"\"\"\n",
    "    image_copy = image.copy()\n",
    "    height, width = image_copy.shape[:2]\n",
    "    \n",
    "    # ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã‚’æç”»\n",
    "    for i, (y, x, conf) in enumerate(keypoints):\n",
    "        if conf > confidence_threshold:\n",
    "            # åº§æ¨™ã‚’ç”»åƒã‚µã‚¤ã‚ºã«ã‚¹ã‚±ãƒ¼ãƒ«\n",
    "            x_coord = int(x * width)\n",
    "            y_coord = int(y * height)\n",
    "            \n",
    "            # ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã‚’å††ã§æç”»\n",
    "            cv2.circle(image_copy, (x_coord, y_coord), 5, (0, 255, 0), -1)\n",
    "            cv2.circle(image_copy, (x_coord, y_coord), 7, (0, 0, 255), 2)\n",
    "    \n",
    "    # ã‚¹ã‚±ãƒ«ãƒˆãƒ³ã‚’æç”»\n",
    "    for connection in CONNECTIONS:\n",
    "        point1_idx, point2_idx = connection\n",
    "        \n",
    "        y1, x1, conf1 = keypoints[point1_idx]\n",
    "        y2, x2, conf2 = keypoints[point2_idx]\n",
    "        \n",
    "        if conf1 > confidence_threshold and conf2 > confidence_threshold:\n",
    "            x1_coord = int(x1 * width)\n",
    "            y1_coord = int(y1 * height)\n",
    "            x2_coord = int(x2 * width)\n",
    "            y2_coord = int(y2 * height)\n",
    "            \n",
    "            cv2.line(image_copy, (x1_coord, y1_coord), (x2_coord, y2_coord), (255, 0, 0), 2)\n",
    "    \n",
    "    return image_copy\n",
    "\n",
    "print(\"âœ… ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆå®šç¾©ã¨æç”»é–¢æ•°ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435d44c8",
   "metadata": {},
   "source": [
    "## 5. å§¿å‹¢æ¨å®šå®Ÿè¡Œé–¢æ•°\n",
    "\n",
    "ç”»åƒã«å¯¾ã—ã¦MoveNet Lightningã§å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°ã‚’å®šç¾©ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1d013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pose_estimation(image):\n",
    "    \"\"\"\n",
    "    å˜ä¸€ç”»åƒã«å¯¾ã—ã¦å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°\n",
    "    \"\"\"\n",
    "    # ç”»åƒã‚’192x192ã«ãƒªã‚µã‚¤ã‚ºï¼ˆMoveNet Lightningã®å…¥åŠ›ã‚µã‚¤ã‚ºï¼‰\n",
    "    input_image = tf.expand_dims(image, axis=0)\n",
    "    input_image = tf.cast(tf.image.resize_with_pad(input_image, 192, 192), dtype=tf.int32)\n",
    "    \n",
    "    # å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œ\n",
    "    outputs = movenet_fn(input_image)\n",
    "    keypoints = outputs['output_0'].numpy()\n",
    "    \n",
    "    # çµæœã‚’è¿”ã™ (17å€‹ã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ Ã— [y, x, confidence])\n",
    "    return keypoints[0, 0, :, :]\n",
    "\n",
    "def analyze_pose_results(keypoints, confidence_threshold=0.3):\n",
    "    \"\"\"\n",
    "    å§¿å‹¢æ¨å®šçµæœã‚’åˆ†æã—ã€æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®æƒ…å ±ã‚’è¡¨ç¤º\n",
    "    \"\"\"\n",
    "    detected_points = []\n",
    "    \n",
    "    for name, idx in KEYPOINT_DICT.items():\n",
    "        y, x, conf = keypoints[idx]\n",
    "        if conf > confidence_threshold:\n",
    "            detected_points.append((name, conf))\n",
    "    \n",
    "    print(f\"ğŸ¯ æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ•°: {len(detected_points)}/17\")\n",
    "    print(\"ğŸ“Š æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ:\")\n",
    "    \n",
    "    for name, conf in sorted(detected_points, key=lambda x: x[1], reverse=True):\n",
    "        print(f\"   - {name}: {conf:.3f}\")\n",
    "    \n",
    "    return detected_points\n",
    "\n",
    "print(\"âœ… å§¿å‹¢æ¨å®šå®Ÿè¡Œé–¢æ•°ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14519fbc",
   "metadata": {},
   "source": [
    "## 6. ãƒ†ã‚¹ãƒˆç”»åƒã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¨æº–å‚™\n",
    "\n",
    "Google Colabã«ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cdd89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "print(\"ğŸ“ å§¿å‹¢æ¨å®šã‚’è¡Œã„ãŸã„ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„...\")\n",
    "print(\"ğŸ’¡ äººç‰©ãŒå†™ã£ã¦ã„ã‚‹ç”»åƒã‚’é¸æŠã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒã®ãƒ‘ã‚¹ã‚’å–å¾—\n",
    "uploaded_image_paths = list(uploaded.keys())\n",
    "print(f\"âœ… {len(uploaded_image_paths)}å€‹ã®ç”»åƒãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸ\")\n",
    "\n",
    "# ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒã‚’è¡¨ç¤º\n",
    "if uploaded_image_paths:\n",
    "    fig, axes = plt.subplots(1, min(3, len(uploaded_image_paths)), figsize=(15, 5))\n",
    "    if len(uploaded_image_paths) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, image_path in enumerate(uploaded_image_paths[:3]):\n",
    "        img = Image.open(image_path)\n",
    "        if len(uploaded_image_paths) > 1:\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].set_title(f'ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”»åƒ {i+1}: {image_path}')\n",
    "            axes[i].axis('off')\n",
    "        else:\n",
    "            axes[0].imshow(img)\n",
    "            axes[0].set_title(f'ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”»åƒ: {image_path}')\n",
    "            axes[0].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âŒ ç”»åƒãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf97eaad",
   "metadata": {},
   "source": [
    "## 7. å§¿å‹¢æ¨å®šã®å®Ÿè¡Œ\n",
    "\n",
    "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸç”»åƒã«å¯¾ã—ã¦MoveNet Lightningã«ã‚ˆã‚‹å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d92bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œ\n",
    "if uploaded_image_paths:\n",
    "    results_list = []\n",
    "    \n",
    "    for image_path in uploaded_image_paths:\n",
    "        print(f\"ğŸ” {image_path} ã®å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œä¸­...\")\n",
    "        \n",
    "        # ç”»åƒã‚’èª­ã¿è¾¼ã¿\n",
    "        image = cv2.imread(image_path)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # å§¿å‹¢æ¨å®šã‚’å®Ÿè¡Œ\n",
    "        keypoints = run_pose_estimation(image_rgb)\n",
    "        results_list.append((image_rgb, keypoints))\n",
    "        \n",
    "        print(f\"âœ… å§¿å‹¢æ¨å®šå®Œäº†: {image_path}\")\n",
    "        \n",
    "        # çµæœã‚’åˆ†æ\n",
    "        detected_points = analyze_pose_results(keypoints)\n",
    "        print()\n",
    "    \n",
    "    print(\"ğŸ‰ å…¨ã¦ã®ç”»åƒã®å§¿å‹¢æ¨å®šãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
    "else:\n",
    "    print(\"âŒ å§¿å‹¢æ¨å®šã™ã‚‹ç”»åƒãŒã‚ã‚Šã¾ã›ã‚“ã€‚ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29987bd",
   "metadata": {},
   "source": [
    "## 8. å§¿å‹¢æ¨å®šçµæœã®å¯è¦–åŒ–\n",
    "\n",
    "æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã¨ã‚¹ã‚±ãƒ«ãƒˆãƒ³ã‚’å…ƒã®ç”»åƒã«é‡ã­ã¦è¡¨ç¤ºã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7915f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å§¿å‹¢æ¨å®šçµæœã‚’å¯è¦–åŒ–\n",
    "if uploaded_image_paths and results_list:\n",
    "    \n",
    "    # ç”»åƒæ•°ã«å¿œã˜ã¦subplotã‚’èª¿æ•´\n",
    "    num_images = len(uploaded_image_paths)\n",
    "    cols = min(2, num_images)\n",
    "    rows = (num_images + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows * 2, cols, figsize=(15, 8 * rows))\n",
    "    if num_images == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    elif rows == 1:\n",
    "        axes = axes.reshape(2, -1)\n",
    "    \n",
    "    for i, (image_path, (image_rgb, keypoints)) in enumerate(zip(uploaded_image_paths, results_list)):\n",
    "        col = i % cols\n",
    "        \n",
    "        # å…ƒç”»åƒã‚’è¡¨ç¤º\n",
    "        axes[0, col].imshow(image_rgb)\n",
    "        axes[0, col].set_title(f'å…ƒç”»åƒ: {image_path}')\n",
    "        axes[0, col].axis('off')\n",
    "        \n",
    "        # å§¿å‹¢æ¨å®šçµæœã‚’é‡ã­ãŸç”»åƒã‚’è¡¨ç¤º\n",
    "        annotated_image = draw_keypoints_and_skeleton(image_rgb, keypoints)\n",
    "        axes[1, col].imshow(annotated_image)\n",
    "        axes[1, col].set_title(f'å§¿å‹¢æ¨å®šçµæœ: {image_path}')\n",
    "        axes[1, col].axis('off')\n",
    "    \n",
    "    # ä½™ã£ãŸsubplotãŒã‚ã‚Œã°éè¡¨ç¤ºã«ã™ã‚‹\n",
    "    for i in range(num_images, cols):\n",
    "        if i < cols:\n",
    "            axes[0, i].axis('off')\n",
    "            axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"ğŸ¨ å§¿å‹¢æ¨å®šçµæœã®å¯è¦–åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
    "    print(\"ğŸ“Š è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹è¦ç´ ã®èª¬æ˜:\")\n",
    "    print(\"   - ç·‘è‰²ã®å††: æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ\")\n",
    "    print(\"   - èµ¤è‰²ã®å††: ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®å¢ƒç•Œ\")\n",
    "    print(\"   - é’è‰²ã®ç·š: ã‚¹ã‚±ãƒ«ãƒˆãƒ³ï¼ˆéª¨æ ¼ï¼‰\")\n",
    "    print(\"   - ä¿¡é ¼åº¦ãŒä½ã„ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã¯è¡¨ç¤ºã•ã‚Œã¾ã›ã‚“\")\n",
    "else:\n",
    "    print(\"âŒ è¡¨ç¤ºã™ã‚‹çµæœãŒã‚ã‚Šã¾ã›ã‚“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc26d64",
   "metadata": {},
   "source": [
    "## 9. è©³ç´°åˆ†æï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "\n",
    "æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®è©³ç´°æƒ…å ±ã‚’ç¢ºèªã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8cda51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è©³ç´°åˆ†æ\n",
    "if uploaded_image_paths and results_list:\n",
    "    print(\"ğŸ“‹ è©³ç´°åˆ†æçµæœ\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, (image_path, (image_rgb, keypoints)) in enumerate(zip(uploaded_image_paths, results_list)):\n",
    "        print(f\"\\nğŸ–¼ï¸  ç”»åƒ {i+1}: {image_path}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # å„ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®è©³ç´°æƒ…å ±\n",
    "        print(\"ğŸ“ å…¨ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®è©³ç´°:\")\n",
    "        for name, idx in KEYPOINT_DICT.items():\n",
    "            y, x, conf = keypoints[idx]\n",
    "            status = \"âœ… æ¤œå‡º\" if conf > 0.3 else \"âŒ æœªæ¤œå‡º\"\n",
    "            print(f\"   {name:15}: {status} (ä¿¡é ¼åº¦: {conf:.3f}, åº§æ¨™: ({x:.3f}, {y:.3f}))\")\n",
    "        \n",
    "        # æ¤œå‡ºç‡ã®è¨ˆç®—\n",
    "        detected_count = sum(1 for _, _, conf in keypoints if conf > 0.3)\n",
    "        detection_rate = detected_count / 17 * 100\n",
    "        print(f\"\\nğŸ“Š æ¤œå‡ºç‡: {detection_rate:.1f}% ({detected_count}/17 ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ)\")\n",
    "        \n",
    "        # å¹³å‡ä¿¡é ¼åº¦\n",
    "        avg_confidence = np.mean([conf for _, _, conf in keypoints if conf > 0.3])\n",
    "        print(f\"ğŸ“ˆ å¹³å‡ä¿¡é ¼åº¦: {avg_confidence:.3f}\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(\"âœ… è©³ç´°åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
    "else:\n",
    "    print(\"âŒ åˆ†æã™ã‚‹çµæœãŒã‚ã‚Šã¾ã›ã‚“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125549b",
   "metadata": {},
   "source": [
    "## ğŸ¯ ä½¿ç”¨æ–¹æ³•ã®ã¾ã¨ã‚\n",
    "\n",
    "1. **ã‚»ãƒ«ã‚’é †ç•ªã«å®Ÿè¡Œ**: ä¸Šã‹ã‚‰é †ç•ªã«ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\n",
    "2. **ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**: ã‚»ãƒ«6ã§äººç‰©ãŒå†™ã£ãŸç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„\n",
    "3. **çµæœã‚’ç¢ºèª**: ã‚»ãƒ«8ã§å§¿å‹¢æ¨å®šçµæœã‚’å¯è¦–åŒ–ã—ã¦ç¢ºèªã§ãã¾ã™\n",
    "4. **è©³ç´°åˆ†æ**: ã‚»ãƒ«9ã§å„ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®è©³ç´°æƒ…å ±ã‚’ç¢ºèªã§ãã¾ã™\n",
    "\n",
    "## ğŸ“ æ³¨æ„äº‹é …\n",
    "\n",
    "- **æœ€é©ãªç”»åƒ**: äººç‰©ãŒæ˜ç¢ºã«å†™ã£ã¦ã„ã‚‹ç”»åƒã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™\n",
    "- **ä¿¡é ¼åº¦é–¾å€¤**: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯0.3ã§ã™ãŒã€å¿…è¦ã«å¿œã˜ã¦èª¿æ•´ã§ãã¾ã™\n",
    "- **ãƒ¢ãƒ‡ãƒ«ã®åˆ¶é™**: å˜ä¸€äººç‰©ã®å§¿å‹¢æ¨å®šã«ã®ã¿å¯¾å¿œã—ã¦ã„ã¾ã™\n",
    "- **GPUåˆ©ç”¨**: Google Colabã§GPUãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã‚ˆã‚Šé«˜é€Ÿã«å‹•ä½œã—ã¾ã™\n",
    "\n",
    "## ğŸ”— å‚è€ƒãƒªãƒ³ã‚¯\n",
    "\n",
    "- [MoveNet: Ultra fast and accurate pose detection model](https://blog.tensorflow.org/2021/05/next-generation-pose-detection-with-movenet-and-tensorflowjs.html)\n",
    "- [TensorFlow Hub - MoveNet](https://tfhub.dev/google/movenet/singlepose/lightning/4)\n",
    "- [TensorFlow Pose Estimation Guide](https://www.tensorflow.org/lite/examples/pose_estimation/overview)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
