{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0b4e435",
   "metadata": {},
   "source": [
    "# MoveNet Lightningによる姿勢推定 - Google Colab版\n",
    "\n",
    "このノートブックでは、TensorFlow HubのMoveNet Lightningモデルを使用してリアルタイム姿勢推定を行います。MoveNet Lightningは軽量で高速な姿勢推定モデルです。\n",
    "\n",
    "## 特徴\n",
    "- 高速なリアルタイム姿勢推定\n",
    "- 17個のキーポイントの検出\n",
    "- 軽量モデル（Lightning版）\n",
    "- 画像・動画両方に対応\n",
    "- 信頼度スコア付きの結果\n",
    "\n",
    "## 必要な環境\n",
    "- Google Colab（推奨）\n",
    "- Python 3.8以上\n",
    "- TensorFlow 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d38d25",
   "metadata": {},
   "source": [
    "## 1. 必要なライブラリのインストール\n",
    "\n",
    "MoveNet Lightningを使用するために必要なライブラリをインストールします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528f3176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlowとTensorFlow Hubをインストール\n",
    "!pip install tensorflow tensorflow-hub\n",
    "\n",
    "# その他の必要なライブラリをインストール\n",
    "!pip install opencv-python-headless pillow matplotlib numpy\n",
    "\n",
    "# インストール完了の確認\n",
    "print(\"✅ 全てのライブラリのインストールが完了しました！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae8316b",
   "metadata": {},
   "source": [
    "## 2. ライブラリのインポートと設定\n",
    "\n",
    "姿勢推定に必要なライブラリをインポートします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7da0552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリをインポート\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "from google.colab import files\n",
    "import io\n",
    "\n",
    "# GPU利用可能性の確認\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(\"✅ 全てのライブラリのインポートが完了しました！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf4d4cd",
   "metadata": {},
   "source": [
    "## 3. MoveNet Lightningモデルの読み込み\n",
    "\n",
    "TensorFlow HubからMoveNet Lightningモデルを読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a25c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MoveNet Lightning モデルをTensorFlow Hubから読み込み\n",
    "model_url = \"https://tfhub.dev/google/movenet/singlepose/lightning/4\"\n",
    "print(\"🔄 MoveNet Lightningモデルを読み込み中...\")\n",
    "\n",
    "# モデルをロード\n",
    "movenet = hub.load(model_url)\n",
    "movenet_fn = movenet.signatures['serving_default']\n",
    "\n",
    "print(\"✅ MoveNet Lightningモデルの読み込みが完了しました！\")\n",
    "print(\"📊 モデル情報:\")\n",
    "print(\"   - 入力サイズ: 192x192\")\n",
    "print(\"   - 検出キーポイント数: 17個\")\n",
    "print(\"   - 出力形式: [y, x, confidence] × 17\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab3b756",
   "metadata": {},
   "source": [
    "## 4. キーポイント定義と描画関数\n",
    "\n",
    "MoveNetが検出する17個のキーポイントの定義と、結果を可視化するための関数を定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69202987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MoveNetのキーポイント定義（17個）\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "# スケルトンの接続関係\n",
    "CONNECTIONS = [\n",
    "    # 顔\n",
    "    (0, 1), (0, 2), (1, 3), (2, 4),\n",
    "    # 胴体\n",
    "    (5, 6), (5, 11), (6, 12), (11, 12),\n",
    "    # 左腕\n",
    "    (5, 7), (7, 9),\n",
    "    # 右腕\n",
    "    (6, 8), (8, 10),\n",
    "    # 左脚\n",
    "    (11, 13), (13, 15),\n",
    "    # 右脚\n",
    "    (12, 14), (14, 16)\n",
    "]\n",
    "\n",
    "def draw_keypoints_and_skeleton(image, keypoints, confidence_threshold=0.3):\n",
    "    \"\"\"\n",
    "    画像にキーポイントとスケルトンを描画する関数\n",
    "    \"\"\"\n",
    "    image_copy = image.copy()\n",
    "    height, width = image_copy.shape[:2]\n",
    "    \n",
    "    # キーポイントを描画\n",
    "    for i, (y, x, conf) in enumerate(keypoints):\n",
    "        if conf > confidence_threshold:\n",
    "            # 座標を画像サイズにスケール\n",
    "            x_coord = int(x * width)\n",
    "            y_coord = int(y * height)\n",
    "            \n",
    "            # キーポイントを円で描画\n",
    "            cv2.circle(image_copy, (x_coord, y_coord), 5, (0, 255, 0), -1)\n",
    "            cv2.circle(image_copy, (x_coord, y_coord), 7, (0, 0, 255), 2)\n",
    "    \n",
    "    # スケルトンを描画\n",
    "    for connection in CONNECTIONS:\n",
    "        point1_idx, point2_idx = connection\n",
    "        \n",
    "        y1, x1, conf1 = keypoints[point1_idx]\n",
    "        y2, x2, conf2 = keypoints[point2_idx]\n",
    "        \n",
    "        if conf1 > confidence_threshold and conf2 > confidence_threshold:\n",
    "            x1_coord = int(x1 * width)\n",
    "            y1_coord = int(y1 * height)\n",
    "            x2_coord = int(x2 * width)\n",
    "            y2_coord = int(y2 * height)\n",
    "            \n",
    "            cv2.line(image_copy, (x1_coord, y1_coord), (x2_coord, y2_coord), (255, 0, 0), 2)\n",
    "    \n",
    "    return image_copy\n",
    "\n",
    "print(\"✅ キーポイント定義と描画関数の準備が完了しました！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cc6526",
   "metadata": {},
   "source": [
    "## 5. 姿勢推定実行関数\n",
    "\n",
    "画像に対してMoveNet Lightningで姿勢推定を実行する関数を定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20e9467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pose_estimation(image):\n",
    "    \"\"\"\n",
    "    単一画像に対して姿勢推定を実行する関数\n",
    "    \"\"\"\n",
    "    # 画像を192x192にリサイズ（MoveNet Lightningの入力サイズ）\n",
    "    input_image = tf.expand_dims(image, axis=0)\n",
    "    input_image = tf.cast(tf.image.resize_with_pad(input_image, 192, 192), dtype=tf.int32)\n",
    "    \n",
    "    # 姿勢推定を実行\n",
    "    outputs = movenet_fn(input_image)\n",
    "    keypoints = outputs['output_0'].numpy()\n",
    "    \n",
    "    # 結果を返す (17個のキーポイント × [y, x, confidence])\n",
    "    return keypoints[0, 0, :, :]\n",
    "\n",
    "def analyze_pose_results(keypoints, confidence_threshold=0.3):\n",
    "    \"\"\"\n",
    "    姿勢推定結果を分析し、検出されたキーポイントの情報を表示\n",
    "    \"\"\"\n",
    "    detected_points = []\n",
    "    \n",
    "    for name, idx in KEYPOINT_DICT.items():\n",
    "        y, x, conf = keypoints[idx]\n",
    "        if conf > confidence_threshold:\n",
    "            detected_points.append((name, conf))\n",
    "    \n",
    "    print(f\"🎯 検出されたキーポイント数: {len(detected_points)}/17\")\n",
    "    print(\"📊 検出されたキーポイント:\")\n",
    "    \n",
    "    for name, conf in sorted(detected_points, key=lambda x: x[1], reverse=True):\n",
    "        print(f\"   - {name}: {conf:.3f}\")\n",
    "    \n",
    "    return detected_points\n",
    "\n",
    "print(\"✅ 姿勢推定実行関数の準備が完了しました！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f5b905",
   "metadata": {},
   "source": [
    "## 6. テスト画像のアップロードと準備\n",
    "\n",
    "Google Colabに画像をアップロードして姿勢推定を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3eaa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像ファイルをアップロード\n",
    "print(\"📁 姿勢推定を行いたい画像ファイルを選択してアップロードしてください...\")\n",
    "print(\"💡 人物が写っている画像を選択することをお勧めします\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# アップロードされた画像のパスを取得\n",
    "uploaded_image_paths = list(uploaded.keys())\n",
    "print(f\"✅ {len(uploaded_image_paths)}個の画像がアップロードされました\")\n",
    "\n",
    "# アップロードされた画像を表示\n",
    "if uploaded_image_paths:\n",
    "    fig, axes = plt.subplots(1, min(3, len(uploaded_image_paths)), figsize=(15, 5))\n",
    "    if len(uploaded_image_paths) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, image_path in enumerate(uploaded_image_paths[:3]):\n",
    "        img = Image.open(image_path)\n",
    "        if len(uploaded_image_paths) > 1:\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].set_title(f'アップロード画像 {i+1}: {image_path}')\n",
    "            axes[i].axis('off')\n",
    "        else:\n",
    "            axes[0].imshow(img)\n",
    "            axes[0].set_title(f'アップロード画像: {image_path}')\n",
    "            axes[0].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"❌ 画像がアップロードされませんでした\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0c0120",
   "metadata": {},
   "source": [
    "## 7. 姿勢推定の実行\n",
    "\n",
    "アップロードした画像に対してMoveNet Lightningによる姿勢推定を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578bc152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 姿勢推定を実行\n",
    "if uploaded_image_paths:\n",
    "    results_list = []\n",
    "    \n",
    "    for image_path in uploaded_image_paths:\n",
    "        print(f\"🔍 {image_path} の姿勢推定を実行中...\")\n",
    "        \n",
    "        # 画像を読み込み\n",
    "        image = cv2.imread(image_path)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # 姿勢推定を実行\n",
    "        keypoints = run_pose_estimation(image_rgb)\n",
    "        results_list.append((image_rgb, keypoints))\n",
    "        \n",
    "        print(f\"✅ 姿勢推定完了: {image_path}\")\n",
    "        \n",
    "        # 結果を分析\n",
    "        detected_points = analyze_pose_results(keypoints)\n",
    "        print()\n",
    "    \n",
    "    print(\"🎉 全ての画像の姿勢推定が完了しました！\")\n",
    "else:\n",
    "    print(\"❌ 姿勢推定する画像がありません。画像をアップロードしてください。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a99317",
   "metadata": {},
   "source": [
    "## 8. 姿勢推定結果の可視化\n",
    "\n",
    "検出されたキーポイントとスケルトンを元の画像に重ねて表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1148847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 姿勢推定結果を可視化\n",
    "if uploaded_image_paths and results_list:\n",
    "    \n",
    "    # 画像数に応じてsubplotを調整\n",
    "    num_images = len(uploaded_image_paths)\n",
    "    cols = min(2, num_images)\n",
    "    rows = (num_images + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows * 2, cols, figsize=(15, 8 * rows))\n",
    "    if num_images == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    elif rows == 1:\n",
    "        axes = axes.reshape(2, -1)\n",
    "    \n",
    "    for i, (image_path, (image_rgb, keypoints)) in enumerate(zip(uploaded_image_paths, results_list)):\n",
    "        col = i % cols\n",
    "        \n",
    "        # 元画像を表示\n",
    "        axes[0, col].imshow(image_rgb)\n",
    "        axes[0, col].set_title(f'元画像: {image_path}')\n",
    "        axes[0, col].axis('off')\n",
    "        \n",
    "        # 姿勢推定結果を重ねた画像を表示\n",
    "        annotated_image = draw_keypoints_and_skeleton(image_rgb, keypoints)\n",
    "        axes[1, col].imshow(annotated_image)\n",
    "        axes[1, col].set_title(f'姿勢推定結果: {image_path}')\n",
    "        axes[1, col].axis('off')\n",
    "    \n",
    "    # 余ったsubplotがあれば非表示にする\n",
    "    for i in range(num_images, cols):\n",
    "        if i < cols:\n",
    "            axes[0, i].axis('off')\n",
    "            axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"🎨 姿勢推定結果の可視化が完了しました！\")\n",
    "    print(\"📊 表示されている要素の説明:\")\n",
    "    print(\"   - 緑色の円: 検出されたキーポイント\")\n",
    "    print(\"   - 赤色の円: キーポイントの境界\")\n",
    "    print(\"   - 青色の線: スケルトン（骨格）\")\n",
    "    print(\"   - 信頼度が低いキーポイントは表示されません\")\n",
    "else:\n",
    "    print(\"❌ 表示する結果がありません\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c003292",
   "metadata": {},
   "source": [
    "## 9. 詳細分析（オプション）\n",
    "\n",
    "検出されたキーポイントの詳細情報を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae344a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 詳細分析\n",
    "if uploaded_image_paths and results_list:\n",
    "    print(\"📋 詳細分析結果\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, (image_path, (image_rgb, keypoints)) in enumerate(zip(uploaded_image_paths, results_list)):\n",
    "        print(f\"\\n🖼️  画像 {i+1}: {image_path}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # 各キーポイントの詳細情報\n",
    "        print(\"📍 全キーポイントの詳細:\")\n",
    "        for name, idx in KEYPOINT_DICT.items():\n",
    "            y, x, conf = keypoints[idx]\n",
    "            status = \"✅ 検出\" if conf > 0.3 else \"❌ 未検出\"\n",
    "            print(f\"   {name:15}: {status} (信頼度: {conf:.3f}, 座標: ({x:.3f}, {y:.3f}))\")\n",
    "        \n",
    "        # 検出率の計算\n",
    "        detected_count = sum(1 for _, _, conf in keypoints if conf > 0.3)\n",
    "        detection_rate = detected_count / 17 * 100\n",
    "        print(f\"\\n📊 検出率: {detection_rate:.1f}% ({detected_count}/17 キーポイント)\")\n",
    "        \n",
    "        # 平均信頼度\n",
    "        avg_confidence = np.mean([conf for _, _, conf in keypoints if conf > 0.3])\n",
    "        print(f\"📈 平均信頼度: {avg_confidence:.3f}\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(\"✅ 詳細分析が完了しました！\")\n",
    "else:\n",
    "    print(\"❌ 分析する結果がありません\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0174aa",
   "metadata": {},
   "source": [
    "## 🎯 使用方法のまとめ\n",
    "\n",
    "1. **セルを順番に実行**: 上から順番にセルを実行してください\n",
    "2. **画像をアップロード**: セル6で人物が写った画像をアップロードしてください\n",
    "3. **結果を確認**: セル8で姿勢推定結果を可視化して確認できます\n",
    "4. **詳細分析**: セル9で各キーポイントの詳細情報を確認できます\n",
    "\n",
    "## 📝 注意事項\n",
    "\n",
    "- **最適な画像**: 人物が明確に写っている画像を使用することをお勧めします\n",
    "- **信頼度閾値**: デフォルトは0.3ですが、必要に応じて調整できます\n",
    "- **モデルの制限**: 単一人物の姿勢推定にのみ対応しています\n",
    "- **GPU利用**: Google ColabでGPUランタイムを使用するとより高速に動作します\n",
    "\n",
    "## 🔗 参考リンク\n",
    "\n",
    "- [MoveNet: Ultra fast and accurate pose detection model](https://blog.tensorflow.org/2021/05/next-generation-pose-detection-with-movenet-and-tensorflowjs.html)\n",
    "- [TensorFlow Hub - MoveNet](https://tfhub.dev/google/movenet/singlepose/lightning/4)\n",
    "- [TensorFlow Pose Estimation Guide](https://www.tensorflow.org/lite/examples/pose_estimation/overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3554ffa7",
   "metadata": {},
   "source": [
    "# MoveNet Lightningによる姿勢推定 - Google Colab版\n",
    "\n",
    "このノートブックでは、TensorFlow HubのMoveNet Lightningモデルを使用してリアルタイム姿勢推定を行います。MoveNet Lightningは軽量で高速な姿勢推定モデルです。\n",
    "\n",
    "## 特徴\n",
    "- 高速なリアルタイム姿勢推定\n",
    "- 17個のキーポイントの検出\n",
    "- 軽量モデル（Lightning版）\n",
    "- 画像・動画両方に対応\n",
    "- 信頼度スコア付きの結果\n",
    "\n",
    "## 必要な環境\n",
    "- Google Colab（推奨）\n",
    "- Python 3.8以上\n",
    "- TensorFlow 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026f3a6e",
   "metadata": {},
   "source": [
    "## 1. 必要なライブラリのインストール\n",
    "\n",
    "MoveNet Lightningを使用するために必要なライブラリをインストールします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6ae8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlowとTensorFlow Hubをインストール\n",
    "!pip install tensorflow tensorflow-hub\n",
    "\n",
    "# その他の必要なライブラリをインストール\n",
    "!pip install opencv-python-headless pillow matplotlib numpy\n",
    "\n",
    "# インストール完了の確認\n",
    "print(\"✅ 全てのライブラリのインストールが完了しました！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ec1aca",
   "metadata": {},
   "source": [
    "## 2. ライブラリのインポートと設定\n",
    "\n",
    "姿勢推定に必要なライブラリをインポートします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada36c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリをインポート\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "from google.colab import files\n",
    "import io\n",
    "\n",
    "# GPU利用可能性の確認\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(\"✅ 全てのライブラリのインポートが完了しました！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4839fe8f",
   "metadata": {},
   "source": [
    "## 3. MoveNet Lightningモデルの読み込み\n",
    "\n",
    "TensorFlow HubからMoveNet Lightningモデルを読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14430130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MoveNet Lightning モデルをTensorFlow Hubから読み込み\n",
    "model_url = \"https://tfhub.dev/google/movenet/singlepose/lightning/4\"\n",
    "print(\"🔄 MoveNet Lightningモデルを読み込み中...\")\n",
    "\n",
    "# モデルをロード\n",
    "movenet = hub.load(model_url)\n",
    "movenet_fn = movenet.signatures['serving_default']\n",
    "\n",
    "print(\"✅ MoveNet Lightningモデルの読み込みが完了しました！\")\n",
    "print(\"📊 モデル情報:\")\n",
    "print(\"   - 入力サイズ: 192x192\")\n",
    "print(\"   - 検出キーポイント数: 17個\")\n",
    "print(\"   - 出力形式: [y, x, confidence] × 17\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509761f1",
   "metadata": {},
   "source": [
    "## 4. キーポイント定義と描画関数\n",
    "\n",
    "MoveNetが検出する17個のキーポイントの定義と、結果を可視化するための関数を定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db8b9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MoveNetのキーポイント定義（17個）\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "# スケルトンの接続関係\n",
    "CONNECTIONS = [\n",
    "    # 顔\n",
    "    (0, 1), (0, 2), (1, 3), (2, 4),\n",
    "    # 胴体\n",
    "    (5, 6), (5, 11), (6, 12), (11, 12),\n",
    "    # 左腕\n",
    "    (5, 7), (7, 9),\n",
    "    # 右腕\n",
    "    (6, 8), (8, 10),\n",
    "    # 左脚\n",
    "    (11, 13), (13, 15),\n",
    "    # 右脚\n",
    "    (12, 14), (14, 16)\n",
    "]\n",
    "\n",
    "def draw_keypoints_and_skeleton(image, keypoints, confidence_threshold=0.3):\n",
    "    \"\"\"\n",
    "    画像にキーポイントとスケルトンを描画する関数\n",
    "    \"\"\"\n",
    "    image_copy = image.copy()\n",
    "    height, width = image_copy.shape[:2]\n",
    "    \n",
    "    # キーポイントを描画\n",
    "    for i, (y, x, conf) in enumerate(keypoints):\n",
    "        if conf > confidence_threshold:\n",
    "            # 座標を画像サイズにスケール\n",
    "            x_coord = int(x * width)\n",
    "            y_coord = int(y * height)\n",
    "            \n",
    "            # キーポイントを円で描画\n",
    "            cv2.circle(image_copy, (x_coord, y_coord), 5, (0, 255, 0), -1)\n",
    "            cv2.circle(image_copy, (x_coord, y_coord), 7, (0, 0, 255), 2)\n",
    "    \n",
    "    # スケルトンを描画\n",
    "    for connection in CONNECTIONS:\n",
    "        point1_idx, point2_idx = connection\n",
    "        \n",
    "        y1, x1, conf1 = keypoints[point1_idx]\n",
    "        y2, x2, conf2 = keypoints[point2_idx]\n",
    "        \n",
    "        if conf1 > confidence_threshold and conf2 > confidence_threshold:\n",
    "            x1_coord = int(x1 * width)\n",
    "            y1_coord = int(y1 * height)\n",
    "            x2_coord = int(x2 * width)\n",
    "            y2_coord = int(y2 * height)\n",
    "            \n",
    "            cv2.line(image_copy, (x1_coord, y1_coord), (x2_coord, y2_coord), (255, 0, 0), 2)\n",
    "    \n",
    "    return image_copy\n",
    "\n",
    "print(\"✅ キーポイント定義と描画関数の準備が完了しました！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61829741",
   "metadata": {},
   "source": [
    "## 5. 姿勢推定実行関数\n",
    "\n",
    "画像に対してMoveNet Lightningで姿勢推定を実行する関数を定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafd995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pose_estimation(image):\n",
    "    \"\"\"\n",
    "    単一画像に対して姿勢推定を実行する関数\n",
    "    \"\"\"\n",
    "    # 画像を192x192にリサイズ（MoveNet Lightningの入力サイズ）\n",
    "    input_image = tf.expand_dims(image, axis=0)\n",
    "    input_image = tf.cast(tf.image.resize_with_pad(input_image, 192, 192), dtype=tf.int32)\n",
    "    \n",
    "    # 姿勢推定を実行\n",
    "    outputs = movenet_fn(input_image)\n",
    "    keypoints = outputs['output_0'].numpy()\n",
    "    \n",
    "    # 結果を返す (17個のキーポイント × [y, x, confidence])\n",
    "    return keypoints[0, 0, :, :]\n",
    "\n",
    "def analyze_pose_results(keypoints, confidence_threshold=0.3):\n",
    "    \"\"\"\n",
    "    姿勢推定結果を分析し、検出されたキーポイントの情報を表示\n",
    "    \"\"\"\n",
    "    detected_points = []\n",
    "    \n",
    "    for name, idx in KEYPOINT_DICT.items():\n",
    "        y, x, conf = keypoints[idx]\n",
    "        if conf > confidence_threshold:\n",
    "            detected_points.append((name, conf))\n",
    "    \n",
    "    print(f\"🎯 検出されたキーポイント数: {len(detected_points)}/17\")\n",
    "    print(\"📊 検出されたキーポイント:\")\n",
    "    \n",
    "    for name, conf in sorted(detected_points, key=lambda x: x[1], reverse=True):\n",
    "        print(f\"   - {name}: {conf:.3f}\")\n",
    "    \n",
    "    return detected_points\n",
    "\n",
    "print(\"✅ 姿勢推定実行関数の準備が完了しました！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607e9cc5",
   "metadata": {},
   "source": [
    "## 6. テスト画像のアップロードと準備\n",
    "\n",
    "Google Colabに画像をアップロードして姿勢推定を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef2d240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像ファイルをアップロード\n",
    "print(\"📁 姿勢推定を行いたい画像ファイルを選択してアップロードしてください...\")\n",
    "print(\"💡 人物が写っている画像を選択することをお勧めします\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# アップロードされた画像のパスを取得\n",
    "uploaded_image_paths = list(uploaded.keys())\n",
    "print(f\"✅ {len(uploaded_image_paths)}個の画像がアップロードされました\")\n",
    "\n",
    "# アップロードされた画像を表示\n",
    "if uploaded_image_paths:\n",
    "    fig, axes = plt.subplots(1, min(3, len(uploaded_image_paths)), figsize=(15, 5))\n",
    "    if len(uploaded_image_paths) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, image_path in enumerate(uploaded_image_paths[:3]):\n",
    "        img = Image.open(image_path)\n",
    "        if len(uploaded_image_paths) > 1:\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].set_title(f'アップロード画像 {i+1}: {image_path}')\n",
    "            axes[i].axis('off')\n",
    "        else:\n",
    "            axes[0].imshow(img)\n",
    "            axes[0].set_title(f'アップロード画像: {image_path}')\n",
    "            axes[0].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"❌ 画像がアップロードされませんでした\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83800ca",
   "metadata": {},
   "source": [
    "## 7. 姿勢推定の実行\n",
    "\n",
    "アップロードした画像に対してMoveNet Lightningによる姿勢推定を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e722dce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 姿勢推定を実行\n",
    "if uploaded_image_paths:\n",
    "    results_list = []\n",
    "    \n",
    "    for image_path in uploaded_image_paths:\n",
    "        print(f\"🔍 {image_path} の姿勢推定を実行中...\")\n",
    "        \n",
    "        # 画像を読み込み\n",
    "        image = cv2.imread(image_path)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # 姿勢推定を実行\n",
    "        keypoints = run_pose_estimation(image_rgb)\n",
    "        results_list.append((image_rgb, keypoints))\n",
    "        \n",
    "        print(f\"✅ 姿勢推定完了: {image_path}\")\n",
    "        \n",
    "        # 結果を分析\n",
    "        detected_points = analyze_pose_results(keypoints)\n",
    "        print()\n",
    "    \n",
    "    print(\"🎉 全ての画像の姿勢推定が完了しました！\")\n",
    "else:\n",
    "    print(\"❌ 姿勢推定する画像がありません。画像をアップロードしてください。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60218de2",
   "metadata": {},
   "source": [
    "## 8. 姿勢推定結果の可視化\n",
    "\n",
    "検出されたキーポイントとスケルトンを元の画像に重ねて表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7608ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 姿勢推定結果を可視化\n",
    "if uploaded_image_paths and results_list:\n",
    "    \n",
    "    # 画像数に応じてsubplotを調整\n",
    "    num_images = len(uploaded_image_paths)\n",
    "    cols = min(2, num_images)\n",
    "    rows = (num_images + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows * 2, cols, figsize=(15, 8 * rows))\n",
    "    if num_images == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    elif rows == 1:\n",
    "        axes = axes.reshape(2, -1)\n",
    "    \n",
    "    for i, (image_path, (image_rgb, keypoints)) in enumerate(zip(uploaded_image_paths, results_list)):\n",
    "        col = i % cols\n",
    "        \n",
    "        # 元画像を表示\n",
    "        axes[0, col].imshow(image_rgb)\n",
    "        axes[0, col].set_title(f'元画像: {image_path}')\n",
    "        axes[0, col].axis('off')\n",
    "        \n",
    "        # 姿勢推定結果を重ねた画像を表示\n",
    "        annotated_image = draw_keypoints_and_skeleton(image_rgb, keypoints)\n",
    "        axes[1, col].imshow(annotated_image)\n",
    "        axes[1, col].set_title(f'姿勢推定結果: {image_path}')\n",
    "        axes[1, col].axis('off')\n",
    "    \n",
    "    # 余ったsubplotがあれば非表示にする\n",
    "    for i in range(num_images, cols):\n",
    "        if i < cols:\n",
    "            axes[0, i].axis('off')\n",
    "            axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"🎨 姿勢推定結果の可視化が完了しました！\")\n",
    "    print(\"📊 表示されている要素の説明:\")\n",
    "    print(\"   - 緑色の円: 検出されたキーポイント\")\n",
    "    print(\"   - 赤色の円: キーポイントの境界\")\n",
    "    print(\"   - 青色の線: スケルトン（骨格）\")\n",
    "    print(\"   - 信頼度が低いキーポイントは表示されません\")\n",
    "else:\n",
    "    print(\"❌ 表示する結果がありません\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe272958",
   "metadata": {},
   "source": [
    "## 9. 詳細分析（オプション）\n",
    "\n",
    "検出されたキーポイントの詳細情報を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64189272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 詳細分析\n",
    "if uploaded_image_paths and results_list:\n",
    "    print(\"📋 詳細分析結果\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, (image_path, (image_rgb, keypoints)) in enumerate(zip(uploaded_image_paths, results_list)):\n",
    "        print(f\"\\n🖼️  画像 {i+1}: {image_path}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # 各キーポイントの詳細情報\n",
    "        print(\"📍 全キーポイントの詳細:\")\n",
    "        for name, idx in KEYPOINT_DICT.items():\n",
    "            y, x, conf = keypoints[idx]\n",
    "            status = \"✅ 検出\" if conf > 0.3 else \"❌ 未検出\"\n",
    "            print(f\"   {name:15}: {status} (信頼度: {conf:.3f}, 座標: ({x:.3f}, {y:.3f}))\")\n",
    "        \n",
    "        # 検出率の計算\n",
    "        detected_count = sum(1 for _, _, conf in keypoints if conf > 0.3)\n",
    "        detection_rate = detected_count / 17 * 100\n",
    "        print(f\"\\n📊 検出率: {detection_rate:.1f}% ({detected_count}/17 キーポイント)\")\n",
    "        \n",
    "        # 平均信頼度\n",
    "        avg_confidence = np.mean([conf for _, _, conf in keypoints if conf > 0.3])\n",
    "        print(f\"📈 平均信頼度: {avg_confidence:.3f}\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(\"✅ 詳細分析が完了しました！\")\n",
    "else:\n",
    "    print(\"❌ 分析する結果がありません\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d806b74",
   "metadata": {},
   "source": [
    "## 🎯 使用方法のまとめ\n",
    "\n",
    "1. **順番に実行**: 上から順番にセルを実行してください\n",
    "2. **画像をアップロード**: 6番目のセルで人物が写った画像をアップロードしてください\n",
    "3. **結果を確認**: 8番目のセルで姿勢推定結果を可視化して確認できます\n",
    "4. **詳細分析**: 9番目のセルで各キーポイントの詳細情報を確認できます\n",
    "\n",
    "## 📝 注意事項\n",
    "\n",
    "- **最適な画像**: 人物が明確に写っている画像を使用することをお勧めします\n",
    "- **信頼度閾値**: デフォルトは0.3ですが、必要に応じて調整できます\n",
    "- **モデルの制限**: 単一人物の姿勢推定にのみ対応しています\n",
    "- **GPU利用**: Google ColabでGPUランタイムを使用するとより高速に動作します\n",
    "\n",
    "## 🔗 参考リンク\n",
    "\n",
    "- [MoveNet: Ultra fast and accurate pose detection model](https://blog.tensorflow.org/2021/05/next-generation-pose-detection-with-movenet-and-tensorflowjs.html)\n",
    "- [TensorFlow Hub - MoveNet](https://tfhub.dev/google/movenet/singlepose/lightning/4)\n",
    "- [TensorFlow Pose Estimation Guide](https://www.tensorflow.org/lite/examples/pose_estimation/overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86bb82b",
   "metadata": {},
   "source": [
    "# MoveNet Lightningによる姿勢推定 - Google Colab版\n",
    "\n",
    "このノートブックでは、TensorFlow HubのMoveNet Lightningモデルを使用してリアルタイム姿勢推定を行います。MoveNet Lightningは軽量で高速な姿勢推定モデルです。\n",
    "\n",
    "## 特徴\n",
    "- 高速なリアルタイム姿勢推定\n",
    "- 17個のキーポイントの検出\n",
    "- 軽量モデル（Lightning版）\n",
    "- 画像・動画両方に対応\n",
    "- 信頼度スコア付きの結果\n",
    "\n",
    "## 必要な環境\n",
    "- Google Colab（推奨）\n",
    "- Python 3.8以上\n",
    "- TensorFlow 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76559edc",
   "metadata": {},
   "source": [
    "## 1. 必要なライブラリのインストール\n",
    "\n",
    "MoveNet Lightningを使用するために必要なライブラリをインストールします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8420144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlowとTensorFlow Hubをインストール\n",
    "!pip install tensorflow tensorflow-hub\n",
    "\n",
    "# その他の必要なライブラリをインストール\n",
    "!pip install opencv-python-headless pillow matplotlib numpy\n",
    "\n",
    "# インストール完了の確認\n",
    "print(\"✅ 全てのライブラリのインストールが完了しました！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746f1e1a",
   "metadata": {},
   "source": [
    "## 2. ライブラリのインポートと設定\n",
    "\n",
    "姿勢推定に必要なライブラリをインポートします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a226fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリをインポート\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "from google.colab import files\n",
    "import io\n",
    "\n",
    "# GPU利用可能性の確認\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(\"✅ 全てのライブラリのインポートが完了しました！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e00b3b8",
   "metadata": {},
   "source": [
    "## 3. MoveNet Lightningモデルの読み込み\n",
    "\n",
    "TensorFlow HubからMoveNet Lightningモデルを読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d18ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MoveNet Lightning モデルをTensorFlow Hubから読み込み\n",
    "model_url = \"https://tfhub.dev/google/movenet/singlepose/lightning/4\"\n",
    "print(\"🔄 MoveNet Lightningモデルを読み込み中...\")\n",
    "\n",
    "# モデルをロード\n",
    "movenet = hub.load(model_url)\n",
    "movenet_fn = movenet.signatures['serving_default']\n",
    "\n",
    "print(\"✅ MoveNet Lightningモデルの読み込みが完了しました！\")\n",
    "print(\"📊 モデル情報:\")\n",
    "print(\"   - 入力サイズ: 192x192\")\n",
    "print(\"   - 検出キーポイント数: 17個\")\n",
    "print(\"   - 出力形式: [y, x, confidence] × 17\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e3e842",
   "metadata": {},
   "source": [
    "## 4. キーポイント定義と描画関数\n",
    "\n",
    "MoveNetが検出する17個のキーポイントの定義と、結果を可視化するための関数を定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7631f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MoveNetのキーポイント定義（17個）\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "# スケルトンの接続関係\n",
    "CONNECTIONS = [\n",
    "    # 顔\n",
    "    (0, 1), (0, 2), (1, 3), (2, 4),\n",
    "    # 胴体\n",
    "    (5, 6), (5, 11), (6, 12), (11, 12),\n",
    "    # 左腕\n",
    "    (5, 7), (7, 9),\n",
    "    # 右腕\n",
    "    (6, 8), (8, 10),\n",
    "    # 左脚\n",
    "    (11, 13), (13, 15),\n",
    "    # 右脚\n",
    "    (12, 14), (14, 16)\n",
    "]\n",
    "\n",
    "def draw_keypoints_and_skeleton(image, keypoints, confidence_threshold=0.3):\n",
    "    \"\"\"\n",
    "    画像にキーポイントとスケルトンを描画する関数\n",
    "    \"\"\"\n",
    "    image_copy = image.copy()\n",
    "    height, width = image_copy.shape[:2]\n",
    "    \n",
    "    # キーポイントを描画\n",
    "    for i, (y, x, conf) in enumerate(keypoints):\n",
    "        if conf > confidence_threshold:\n",
    "            # 座標を画像サイズにスケール\n",
    "            x_coord = int(x * width)\n",
    "            y_coord = int(y * height)\n",
    "            \n",
    "            # キーポイントを円で描画\n",
    "            cv2.circle(image_copy, (x_coord, y_coord), 5, (0, 255, 0), -1)\n",
    "            cv2.circle(image_copy, (x_coord, y_coord), 7, (0, 0, 255), 2)\n",
    "    \n",
    "    # スケルトンを描画\n",
    "    for connection in CONNECTIONS:\n",
    "        point1_idx, point2_idx = connection\n",
    "        \n",
    "        y1, x1, conf1 = keypoints[point1_idx]\n",
    "        y2, x2, conf2 = keypoints[point2_idx]\n",
    "        \n",
    "        if conf1 > confidence_threshold and conf2 > confidence_threshold:\n",
    "            x1_coord = int(x1 * width)\n",
    "            y1_coord = int(y1 * height)\n",
    "            x2_coord = int(x2 * width)\n",
    "            y2_coord = int(y2 * height)\n",
    "            \n",
    "            cv2.line(image_copy, (x1_coord, y1_coord), (x2_coord, y2_coord), (255, 0, 0), 2)\n",
    "    \n",
    "    return image_copy\n",
    "\n",
    "print(\"✅ キーポイント定義と描画関数の準備が完了しました！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5feb65",
   "metadata": {},
   "source": [
    "## 5. 姿勢推定実行関数\n",
    "\n",
    "画像に対してMoveNet Lightningで姿勢推定を実行する関数を定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ce1560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pose_estimation(image):\n",
    "    \"\"\"\n",
    "    単一画像に対して姿勢推定を実行する関数\n",
    "    \"\"\"\n",
    "    # 画像を192x192にリサイズ（MoveNet Lightningの入力サイズ）\n",
    "    input_image = tf.expand_dims(image, axis=0)\n",
    "    input_image = tf.cast(tf.image.resize_with_pad(input_image, 192, 192), dtype=tf.int32)\n",
    "    \n",
    "    # 姿勢推定を実行\n",
    "    outputs = movenet_fn(input_image)\n",
    "    keypoints = outputs['output_0'].numpy()\n",
    "    \n",
    "    # 結果を返す (17個のキーポイント × [y, x, confidence])\n",
    "    return keypoints[0, 0, :, :]\n",
    "\n",
    "def analyze_pose_results(keypoints, confidence_threshold=0.3):\n",
    "    \"\"\"\n",
    "    姿勢推定結果を分析し、検出されたキーポイントの情報を表示\n",
    "    \"\"\"\n",
    "    detected_points = []\n",
    "    \n",
    "    for name, idx in KEYPOINT_DICT.items():\n",
    "        y, x, conf = keypoints[idx]\n",
    "        if conf > confidence_threshold:\n",
    "            detected_points.append((name, conf))\n",
    "    \n",
    "    print(f\"🎯 検出されたキーポイント数: {len(detected_points)}/17\")\n",
    "    print(\"📊 検出されたキーポイント:\")\n",
    "    \n",
    "    for name, conf in sorted(detected_points, key=lambda x: x[1], reverse=True):\n",
    "        print(f\"   - {name}: {conf:.3f}\")\n",
    "    \n",
    "    return detected_points\n",
    "\n",
    "print(\"✅ 姿勢推定実行関数の準備が完了しました！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc75301",
   "metadata": {},
   "source": [
    "## 6. テスト画像のアップロードと準備\n",
    "\n",
    "Google Colabに画像をアップロードして姿勢推定を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e81641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像ファイルをアップロード\n",
    "print(\"📁 姿勢推定を行いたい画像ファイルを選択してアップロードしてください...\")\n",
    "print(\"💡 人物が写っている画像を選択することをお勧めします\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# アップロードされた画像のパスを取得\n",
    "uploaded_image_paths = list(uploaded.keys())\n",
    "print(f\"✅ {len(uploaded_image_paths)}個の画像がアップロードされました\")\n",
    "\n",
    "# アップロードされた画像を表示\n",
    "if uploaded_image_paths:\n",
    "    fig, axes = plt.subplots(1, min(3, len(uploaded_image_paths)), figsize=(15, 5))\n",
    "    if len(uploaded_image_paths) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, image_path in enumerate(uploaded_image_paths[:3]):\n",
    "        img = Image.open(image_path)\n",
    "        if len(uploaded_image_paths) > 1:\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].set_title(f'アップロード画像 {i+1}: {image_path}')\n",
    "            axes[i].axis('off')\n",
    "        else:\n",
    "            axes[0].imshow(img)\n",
    "            axes[0].set_title(f'アップロード画像: {image_path}')\n",
    "            axes[0].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"❌ 画像がアップロードされませんでした\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b1b8a7",
   "metadata": {},
   "source": [
    "## 7. 姿勢推定の実行\n",
    "\n",
    "アップロードした画像に対してMoveNet Lightningによる姿勢推定を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f447bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 姿勢推定を実行\n",
    "if uploaded_image_paths:\n",
    "    results_list = []\n",
    "    \n",
    "    for image_path in uploaded_image_paths:\n",
    "        print(f\"🔍 {image_path} の姿勢推定を実行中...\")\n",
    "        \n",
    "        # 画像を読み込み\n",
    "        image = cv2.imread(image_path)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # 姿勢推定を実行\n",
    "        keypoints = run_pose_estimation(image_rgb)\n",
    "        results_list.append((image_rgb, keypoints))\n",
    "        \n",
    "        print(f\"✅ 姿勢推定完了: {image_path}\")\n",
    "        \n",
    "        # 結果を分析\n",
    "        detected_points = analyze_pose_results(keypoints)\n",
    "        print()\n",
    "    \n",
    "    print(\"🎉 全ての画像の姿勢推定が完了しました！\")\n",
    "else:\n",
    "    print(\"❌ 姿勢推定する画像がありません。画像をアップロードしてください。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5c52e0",
   "metadata": {},
   "source": [
    "## 8. 姿勢推定結果の可視化\n",
    "\n",
    "検出されたキーポイントとスケルトンを元の画像に重ねて表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3df476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 姿勢推定結果を可視化\n",
    "if uploaded_image_paths and results_list:\n",
    "    \n",
    "    # 画像数に応じてsubplotを調整\n",
    "    num_images = len(uploaded_image_paths)\n",
    "    cols = min(2, num_images)\n",
    "    rows = (num_images + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows * 2, cols, figsize=(15, 8 * rows))\n",
    "    if num_images == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    elif rows == 1:\n",
    "        axes = axes.reshape(2, -1)\n",
    "    \n",
    "    for i, (image_path, (image_rgb, keypoints)) in enumerate(zip(uploaded_image_paths, results_list)):\n",
    "        col = i % cols\n",
    "        \n",
    "        # 元画像を表示\n",
    "        axes[0, col].imshow(image_rgb)\n",
    "        axes[0, col].set_title(f'元画像: {image_path}')\n",
    "        axes[0, col].axis('off')\n",
    "        \n",
    "        # 姿勢推定結果を重ねた画像を表示\n",
    "        annotated_image = draw_keypoints_and_skeleton(image_rgb, keypoints)\n",
    "        axes[1, col].imshow(annotated_image)\n",
    "        axes[1, col].set_title(f'姿勢推定結果: {image_path}')\n",
    "        axes[1, col].axis('off')\n",
    "    \n",
    "    # 余ったsubplotがあれば非表示にする\n",
    "    for i in range(num_images, cols):\n",
    "        if i < cols:\n",
    "            axes[0, i].axis('off')\n",
    "            axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"🎨 姿勢推定結果の可視化が完了しました！\")\n",
    "    print(\"📊 表示されている要素の説明:\")\n",
    "    print(\"   - 緑色の円: 検出されたキーポイント\")\n",
    "    print(\"   - 赤色の円: キーポイントの境界\")\n",
    "    print(\"   - 青色の線: スケルトン（骨格）\")\n",
    "    print(\"   - 信頼度が低いキーポイントは表示されません\")\n",
    "else:\n",
    "    print(\"❌ 表示する結果がありません\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da638b36",
   "metadata": {},
   "source": [
    "## 9. 詳細分析（オプション）\n",
    "\n",
    "検出されたキーポイントの詳細情報を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e172491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 詳細分析\n",
    "if uploaded_image_paths and results_list:\n",
    "    print(\"📋 詳細分析結果\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, (image_path, (image_rgb, keypoints)) in enumerate(zip(uploaded_image_paths, results_list)):\n",
    "        print(f\"\\n🖼️  画像 {i+1}: {image_path}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # 各キーポイントの詳細情報\n",
    "        print(\"📍 全キーポイントの詳細:\")\n",
    "        for name, idx in KEYPOINT_DICT.items():\n",
    "            y, x, conf = keypoints[idx]\n",
    "            status = \"✅ 検出\" if conf > 0.3 else \"❌ 未検出\"\n",
    "            print(f\"   {name:15}: {status} (信頼度: {conf:.3f}, 座標: ({x:.3f}, {y:.3f}))\")\n",
    "        \n",
    "        # 検出率の計算\n",
    "        detected_count = sum(1 for _, _, conf in keypoints if conf > 0.3)\n",
    "        detection_rate = detected_count / 17 * 100\n",
    "        print(f\"\\n📊 検出率: {detection_rate:.1f}% ({detected_count}/17 キーポイント)\")\n",
    "        \n",
    "        # 平均信頼度\n",
    "        avg_confidence = np.mean([conf for _, _, conf in keypoints if conf > 0.3])\n",
    "        print(f\"📈 平均信頼度: {avg_confidence:.3f}\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(\"✅ 詳細分析が完了しました！\")\n",
    "else:\n",
    "    print(\"❌ 分析する結果がありません\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145973f8",
   "metadata": {},
   "source": [
    "## 🎯 使用方法のまとめ\n",
    "\n",
    "1. **セルを順番に実行**: 上から順番にセルを実行してください\n",
    "2. **画像をアップロード**: 6番目のセルで人物が写った画像をアップロードしてください\n",
    "3. **結果を確認**: 8番目のセルで姿勢推定結果を可視化して確認できます\n",
    "4. **詳細分析**: 9番目のセルで各キーポイントの詳細情報を確認できます\n",
    "\n",
    "## 📝 注意事項\n",
    "\n",
    "- **最適な画像**: 人物が明確に写っている画像を使用することをお勧めします\n",
    "- **信頼度閾値**: デフォルトは0.3ですが、必要に応じて調整できます\n",
    "- **モデルの制限**: 単一人物の姿勢推定にのみ対応しています\n",
    "- **GPU利用**: Google ColabでGPUランタイムを使用するとより高速に動作します\n",
    "\n",
    "## 🔗 参考リンク\n",
    "\n",
    "- [MoveNet: Ultra fast and accurate pose detection model](https://blog.tensorflow.org/2021/05/next-generation-pose-detection-with-movenet-and-tensorflowjs.html)\n",
    "- [TensorFlow Hub - MoveNet](https://tfhub.dev/google/movenet/singlepose/lightning/4)\n",
    "- [TensorFlow Pose Estimation Guide](https://www.tensorflow.org/lite/examples/pose_estimation/overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c537f906",
   "metadata": {},
   "source": [
    "# MoveNet Lightningによる姿勢推定 - Google Colab版\n",
    "\n",
    "このノートブックでは、TensorFlow HubのMoveNet Lightningモデルを使用してリアルタイム姿勢推定を行います。MoveNet Lightningは軽量で高速な姿勢推定モデルです。\n",
    "\n",
    "## 特徴\n",
    "- 高速なリアルタイム姿勢推定\n",
    "- 17個のキーポイントの検出\n",
    "- 軽量モデル（Lightning版）\n",
    "- 画像・動画両方に対応\n",
    "- 信頼度スコア付きの結果\n",
    "\n",
    "## 必要な環境\n",
    "- Google Colab（推奨）\n",
    "- Python 3.8以上\n",
    "- TensorFlow 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1df3fc9",
   "metadata": {},
   "source": [
    "## 1. 必要なライブラリのインストール\n",
    "\n",
    "MoveNet Lightningを使用するために必要なライブラリをインストールします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c23f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlowとTensorFlow Hubをインストール\n",
    "!pip install tensorflow tensorflow-hub\n",
    "\n",
    "# その他の必要なライブラリをインストール\n",
    "!pip install opencv-python-headless pillow matplotlib numpy\n",
    "\n",
    "# インストール完了の確認\n",
    "print(\"✅ 全てのライブラリのインストールが完了しました！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58d767a",
   "metadata": {},
   "source": [
    "## 2. ライブラリのインポートと設定\n",
    "\n",
    "姿勢推定に必要なライブラリをインポートします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7150e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリをインポート\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "from google.colab import files\n",
    "import io\n",
    "\n",
    "# GPU利用可能性の確認\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(\"✅ 全てのライブラリのインポートが完了しました！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b5d529",
   "metadata": {},
   "source": [
    "## 3. MoveNet Lightningモデルの読み込み\n",
    "\n",
    "TensorFlow HubからMoveNet Lightningモデルを読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abe4ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MoveNet Lightning モデルをTensorFlow Hubから読み込み\n",
    "model_url = \"https://tfhub.dev/google/movenet/singlepose/lightning/4\"\n",
    "print(\"🔄 MoveNet Lightningモデルを読み込み中...\")\n",
    "\n",
    "# モデルをロード\n",
    "movenet = hub.load(model_url)\n",
    "movenet_fn = movenet.signatures['serving_default']\n",
    "\n",
    "print(\"✅ MoveNet Lightningモデルの読み込みが完了しました！\")\n",
    "print(\"📊 モデル情報:\")\n",
    "print(\"   - 入力サイズ: 192x192\")\n",
    "print(\"   - 検出キーポイント数: 17個\")\n",
    "print(\"   - 出力形式: [y, x, confidence] × 17\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dbabe7",
   "metadata": {},
   "source": [
    "## 4. キーポイント定義と描画関数\n",
    "\n",
    "MoveNetが検出する17個のキーポイントの定義と、結果を可視化するための関数を定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcf83f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MoveNetのキーポイント定義（17個）\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "# スケルトンの接続関係\n",
    "CONNECTIONS = [\n",
    "    # 顔\n",
    "    (0, 1), (0, 2), (1, 3), (2, 4),\n",
    "    # 胴体\n",
    "    (5, 6), (5, 11), (6, 12), (11, 12),\n",
    "    # 左腕\n",
    "    (5, 7), (7, 9),\n",
    "    # 右腕\n",
    "    (6, 8), (8, 10),\n",
    "    # 左脚\n",
    "    (11, 13), (13, 15),\n",
    "    # 右脚\n",
    "    (12, 14), (14, 16)\n",
    "]\n",
    "\n",
    "def draw_keypoints_and_skeleton(image, keypoints, confidence_threshold=0.3):\n",
    "    \"\"\"\n",
    "    画像にキーポイントとスケルトンを描画する関数\n",
    "    \"\"\"\n",
    "    image_copy = image.copy()\n",
    "    height, width = image_copy.shape[:2]\n",
    "    \n",
    "    # キーポイントを描画\n",
    "    for i, (y, x, conf) in enumerate(keypoints):\n",
    "        if conf > confidence_threshold:\n",
    "            # 座標を画像サイズにスケール\n",
    "            x_coord = int(x * width)\n",
    "            y_coord = int(y * height)\n",
    "            \n",
    "            # キーポイントを円で描画\n",
    "            cv2.circle(image_copy, (x_coord, y_coord), 5, (0, 255, 0), -1)\n",
    "            cv2.circle(image_copy, (x_coord, y_coord), 7, (0, 0, 255), 2)\n",
    "    \n",
    "    # スケルトンを描画\n",
    "    for connection in CONNECTIONS:\n",
    "        point1_idx, point2_idx = connection\n",
    "        \n",
    "        y1, x1, conf1 = keypoints[point1_idx]\n",
    "        y2, x2, conf2 = keypoints[point2_idx]\n",
    "        \n",
    "        if conf1 > confidence_threshold and conf2 > confidence_threshold:\n",
    "            x1_coord = int(x1 * width)\n",
    "            y1_coord = int(y1 * height)\n",
    "            x2_coord = int(x2 * width)\n",
    "            y2_coord = int(y2 * height)\n",
    "            \n",
    "            cv2.line(image_copy, (x1_coord, y1_coord), (x2_coord, y2_coord), (255, 0, 0), 2)\n",
    "    \n",
    "    return image_copy\n",
    "\n",
    "print(\"✅ キーポイント定義と描画関数の準備が完了しました！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435d44c8",
   "metadata": {},
   "source": [
    "## 5. 姿勢推定実行関数\n",
    "\n",
    "画像に対してMoveNet Lightningで姿勢推定を実行する関数を定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1d013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pose_estimation(image):\n",
    "    \"\"\"\n",
    "    単一画像に対して姿勢推定を実行する関数\n",
    "    \"\"\"\n",
    "    # 画像を192x192にリサイズ（MoveNet Lightningの入力サイズ）\n",
    "    input_image = tf.expand_dims(image, axis=0)\n",
    "    input_image = tf.cast(tf.image.resize_with_pad(input_image, 192, 192), dtype=tf.int32)\n",
    "    \n",
    "    # 姿勢推定を実行\n",
    "    outputs = movenet_fn(input_image)\n",
    "    keypoints = outputs['output_0'].numpy()\n",
    "    \n",
    "    # 結果を返す (17個のキーポイント × [y, x, confidence])\n",
    "    return keypoints[0, 0, :, :]\n",
    "\n",
    "def analyze_pose_results(keypoints, confidence_threshold=0.3):\n",
    "    \"\"\"\n",
    "    姿勢推定結果を分析し、検出されたキーポイントの情報を表示\n",
    "    \"\"\"\n",
    "    detected_points = []\n",
    "    \n",
    "    for name, idx in KEYPOINT_DICT.items():\n",
    "        y, x, conf = keypoints[idx]\n",
    "        if conf > confidence_threshold:\n",
    "            detected_points.append((name, conf))\n",
    "    \n",
    "    print(f\"🎯 検出されたキーポイント数: {len(detected_points)}/17\")\n",
    "    print(\"📊 検出されたキーポイント:\")\n",
    "    \n",
    "    for name, conf in sorted(detected_points, key=lambda x: x[1], reverse=True):\n",
    "        print(f\"   - {name}: {conf:.3f}\")\n",
    "    \n",
    "    return detected_points\n",
    "\n",
    "print(\"✅ 姿勢推定実行関数の準備が完了しました！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14519fbc",
   "metadata": {},
   "source": [
    "## 6. テスト画像のアップロードと準備\n",
    "\n",
    "Google Colabに画像をアップロードして姿勢推定を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cdd89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像ファイルをアップロード\n",
    "print(\"📁 姿勢推定を行いたい画像ファイルを選択してアップロードしてください...\")\n",
    "print(\"💡 人物が写っている画像を選択することをお勧めします\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# アップロードされた画像のパスを取得\n",
    "uploaded_image_paths = list(uploaded.keys())\n",
    "print(f\"✅ {len(uploaded_image_paths)}個の画像がアップロードされました\")\n",
    "\n",
    "# アップロードされた画像を表示\n",
    "if uploaded_image_paths:\n",
    "    fig, axes = plt.subplots(1, min(3, len(uploaded_image_paths)), figsize=(15, 5))\n",
    "    if len(uploaded_image_paths) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, image_path in enumerate(uploaded_image_paths[:3]):\n",
    "        img = Image.open(image_path)\n",
    "        if len(uploaded_image_paths) > 1:\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].set_title(f'アップロード画像 {i+1}: {image_path}')\n",
    "            axes[i].axis('off')\n",
    "        else:\n",
    "            axes[0].imshow(img)\n",
    "            axes[0].set_title(f'アップロード画像: {image_path}')\n",
    "            axes[0].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"❌ 画像がアップロードされませんでした\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf97eaad",
   "metadata": {},
   "source": [
    "## 7. 姿勢推定の実行\n",
    "\n",
    "アップロードした画像に対してMoveNet Lightningによる姿勢推定を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d92bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 姿勢推定を実行\n",
    "if uploaded_image_paths:\n",
    "    results_list = []\n",
    "    \n",
    "    for image_path in uploaded_image_paths:\n",
    "        print(f\"🔍 {image_path} の姿勢推定を実行中...\")\n",
    "        \n",
    "        # 画像を読み込み\n",
    "        image = cv2.imread(image_path)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # 姿勢推定を実行\n",
    "        keypoints = run_pose_estimation(image_rgb)\n",
    "        results_list.append((image_rgb, keypoints))\n",
    "        \n",
    "        print(f\"✅ 姿勢推定完了: {image_path}\")\n",
    "        \n",
    "        # 結果を分析\n",
    "        detected_points = analyze_pose_results(keypoints)\n",
    "        print()\n",
    "    \n",
    "    print(\"🎉 全ての画像の姿勢推定が完了しました！\")\n",
    "else:\n",
    "    print(\"❌ 姿勢推定する画像がありません。画像をアップロードしてください。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29987bd",
   "metadata": {},
   "source": [
    "## 8. 姿勢推定結果の可視化\n",
    "\n",
    "検出されたキーポイントとスケルトンを元の画像に重ねて表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7915f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 姿勢推定結果を可視化\n",
    "if uploaded_image_paths and results_list:\n",
    "    \n",
    "    # 画像数に応じてsubplotを調整\n",
    "    num_images = len(uploaded_image_paths)\n",
    "    cols = min(2, num_images)\n",
    "    rows = (num_images + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows * 2, cols, figsize=(15, 8 * rows))\n",
    "    if num_images == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    elif rows == 1:\n",
    "        axes = axes.reshape(2, -1)\n",
    "    \n",
    "    for i, (image_path, (image_rgb, keypoints)) in enumerate(zip(uploaded_image_paths, results_list)):\n",
    "        col = i % cols\n",
    "        \n",
    "        # 元画像を表示\n",
    "        axes[0, col].imshow(image_rgb)\n",
    "        axes[0, col].set_title(f'元画像: {image_path}')\n",
    "        axes[0, col].axis('off')\n",
    "        \n",
    "        # 姿勢推定結果を重ねた画像を表示\n",
    "        annotated_image = draw_keypoints_and_skeleton(image_rgb, keypoints)\n",
    "        axes[1, col].imshow(annotated_image)\n",
    "        axes[1, col].set_title(f'姿勢推定結果: {image_path}')\n",
    "        axes[1, col].axis('off')\n",
    "    \n",
    "    # 余ったsubplotがあれば非表示にする\n",
    "    for i in range(num_images, cols):\n",
    "        if i < cols:\n",
    "            axes[0, i].axis('off')\n",
    "            axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"🎨 姿勢推定結果の可視化が完了しました！\")\n",
    "    print(\"📊 表示されている要素の説明:\")\n",
    "    print(\"   - 緑色の円: 検出されたキーポイント\")\n",
    "    print(\"   - 赤色の円: キーポイントの境界\")\n",
    "    print(\"   - 青色の線: スケルトン（骨格）\")\n",
    "    print(\"   - 信頼度が低いキーポイントは表示されません\")\n",
    "else:\n",
    "    print(\"❌ 表示する結果がありません\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc26d64",
   "metadata": {},
   "source": [
    "## 9. 詳細分析（オプション）\n",
    "\n",
    "検出されたキーポイントの詳細情報を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8cda51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 詳細分析\n",
    "if uploaded_image_paths and results_list:\n",
    "    print(\"📋 詳細分析結果\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, (image_path, (image_rgb, keypoints)) in enumerate(zip(uploaded_image_paths, results_list)):\n",
    "        print(f\"\\n🖼️  画像 {i+1}: {image_path}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # 各キーポイントの詳細情報\n",
    "        print(\"📍 全キーポイントの詳細:\")\n",
    "        for name, idx in KEYPOINT_DICT.items():\n",
    "            y, x, conf = keypoints[idx]\n",
    "            status = \"✅ 検出\" if conf > 0.3 else \"❌ 未検出\"\n",
    "            print(f\"   {name:15}: {status} (信頼度: {conf:.3f}, 座標: ({x:.3f}, {y:.3f}))\")\n",
    "        \n",
    "        # 検出率の計算\n",
    "        detected_count = sum(1 for _, _, conf in keypoints if conf > 0.3)\n",
    "        detection_rate = detected_count / 17 * 100\n",
    "        print(f\"\\n📊 検出率: {detection_rate:.1f}% ({detected_count}/17 キーポイント)\")\n",
    "        \n",
    "        # 平均信頼度\n",
    "        avg_confidence = np.mean([conf for _, _, conf in keypoints if conf > 0.3])\n",
    "        print(f\"📈 平均信頼度: {avg_confidence:.3f}\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(\"✅ 詳細分析が完了しました！\")\n",
    "else:\n",
    "    print(\"❌ 分析する結果がありません\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125549b",
   "metadata": {},
   "source": [
    "## 🎯 使用方法のまとめ\n",
    "\n",
    "1. **セルを順番に実行**: 上から順番にセルを実行してください\n",
    "2. **画像をアップロード**: セル6で人物が写った画像をアップロードしてください\n",
    "3. **結果を確認**: セル8で姿勢推定結果を可視化して確認できます\n",
    "4. **詳細分析**: セル9で各キーポイントの詳細情報を確認できます\n",
    "\n",
    "## 📝 注意事項\n",
    "\n",
    "- **最適な画像**: 人物が明確に写っている画像を使用することをお勧めします\n",
    "- **信頼度閾値**: デフォルトは0.3ですが、必要に応じて調整できます\n",
    "- **モデルの制限**: 単一人物の姿勢推定にのみ対応しています\n",
    "- **GPU利用**: Google ColabでGPUランタイムを使用するとより高速に動作します\n",
    "\n",
    "## 🔗 参考リンク\n",
    "\n",
    "- [MoveNet: Ultra fast and accurate pose detection model](https://blog.tensorflow.org/2021/05/next-generation-pose-detection-with-movenet-and-tensorflowjs.html)\n",
    "- [TensorFlow Hub - MoveNet](https://tfhub.dev/google/movenet/singlepose/lightning/4)\n",
    "- [TensorFlow Pose Estimation Guide](https://www.tensorflow.org/lite/examples/pose_estimation/overview)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
